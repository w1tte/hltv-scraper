---
phase: 09-pipeline-orchestration
plan: 03
type: execute
wave: 2
depends_on: ["09-01", "09-02"]
files_modified:
  - src/scraper/pipeline.py
  - src/scraper/cli.py
  - pyproject.toml
autonomous: true

must_haves:
  truths:
    - "Running `hltv-scraper` executes the full pipeline from discovery through performance/economy"
    - "Pipeline runs stages sequentially: discovery -> overview -> map stats -> perf/economy"
    - "Pipeline halts after N consecutive failures"
    - "Ctrl+C finishes current work, prints summary, and exits cleanly"
    - "End-of-run summary shows counts and timing for each stage"
  artifacts:
    - path: "src/scraper/pipeline.py"
      provides: "run_pipeline async function"
      exports: ["run_pipeline"]
      min_lines: 60
    - path: "src/scraper/cli.py"
      provides: "CLI entry point with argparse"
      exports: ["main"]
      min_lines: 40
    - path: "pyproject.toml"
      provides: "console_scripts entry point"
      contains: "hltv-scraper"
  key_links:
    - from: "src/scraper/pipeline.py"
      to: "src/scraper/discovery.py"
      via: "run_discovery call"
      pattern: "run_discovery"
    - from: "src/scraper/pipeline.py"
      to: "src/scraper/match_overview.py"
      via: "run_match_overview call in loop"
      pattern: "run_match_overview"
    - from: "src/scraper/pipeline.py"
      to: "src/scraper/map_stats.py"
      via: "run_map_stats call in loop"
      pattern: "run_map_stats"
    - from: "src/scraper/pipeline.py"
      to: "src/scraper/performance_economy.py"
      via: "run_performance_economy call in loop"
      pattern: "run_performance_economy"
    - from: "src/scraper/cli.py"
      to: "src/scraper/pipeline.py"
      via: "run_pipeline call inside asyncio.run"
      pattern: "run_pipeline"
    - from: "src/scraper/cli.py"
      to: "src/scraper/logging_config.py"
      via: "setup_logging call"
      pattern: "setup_logging"
---

<objective>
Build the pipeline runner function that wires all 4 existing orchestrators into a loop-until-done pipeline, and create the CLI entry point that users invoke to run the scraper.

Purpose: This is the core deliverable of Phase 9 -- the single command that runs the entire scraper end-to-end with resume, incremental mode, progress reporting, and graceful shutdown.
Output: Complete `pipeline.py` (runner function added to existing utility classes), new `cli.py`, updated `pyproject.toml`.
</objective>

<execution_context>
@~/.claude/get-shit-done/workflows/execute-plan.md
@~/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/09-pipeline-orchestration/09-CONTEXT.md
@.planning/phases/09-pipeline-orchestration/09-RESEARCH.md
@.planning/phases/09-pipeline-orchestration/09-01-SUMMARY.md
@.planning/phases/09-pipeline-orchestration/09-02-SUMMARY.md
@src/scraper/config.py
@src/scraper/http_client.py
@src/scraper/db.py
@src/scraper/discovery.py
@src/scraper/match_overview.py
@src/scraper/map_stats.py
@src/scraper/performance_economy.py
@src/scraper/repository.py
@src/scraper/discovery_repository.py
@src/scraper/storage.py
@src/scraper/pipeline.py
@src/scraper/logging_config.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Pipeline runner function</name>
  <files>src/scraper/pipeline.py</files>
  <action>
Add the `run_pipeline` async function to the existing `pipeline.py` file (which already has ShutdownHandler, ConsecutiveFailureTracker, ProgressTracker from Plan 01).

Add these imports at the top of pipeline.py:
```python
from scraper.discovery import run_discovery
from scraper.match_overview import run_match_overview
from scraper.map_stats import run_map_stats
from scraper.performance_economy import run_performance_economy
```

**`run_pipeline` function signature:**
```python
async def run_pipeline(
    client,              # HLTVClient
    match_repo,          # MatchRepository
    discovery_repo,      # DiscoveryRepository
    storage,             # HtmlStorage
    config,              # ScraperConfig
    shutdown,            # ShutdownHandler
    incremental: bool = True,
    force_rescrape: bool = False,
) -> dict:
```

**Implementation:**

1. Create a ConsecutiveFailureTracker with `config.consecutive_failure_threshold`.
2. Create a ProgressTracker (total=0 initially, unknown until discovery completes).
3. Initialize a results dict:
   ```python
   results = {
       "discovery": {},
       "overview": {"parsed": 0, "failed": 0},
       "map_stats": {"parsed": 0, "failed": 0},
       "perf_economy": {"parsed": 0, "failed": 0},
       "halted": False,
       "halt_reason": None,
   }
   ```

4. **Auto-reset failed matches** (CONTEXT.md: "failed matches auto-retry on next run"):
   ```python
   if not force_rescrape:
       reset_count = discovery_repo.reset_failed_matches()
       if reset_count > 0:
           logger.info("Reset %d failed matches to pending", reset_count)
   ```

5. **Stage 1: Discovery**
   ```python
   if not shutdown.is_set:
       logger.info("=== Stage 1: Discovery ===")
       try:
           discovery_stats = await run_discovery(
               client, discovery_repo, storage, config,
               incremental=incremental, shutdown=shutdown,
           )
           results["discovery"] = discovery_stats
           progress.log_stage("Discovery", discovery_stats)
       except Exception as exc:
           logger.error("Discovery failed: %s", exc)
           if failure_tracker.record_failure():
               results["halted"] = True
               results["halt_reason"] = f"Discovery failed: {exc}"
               return results
   ```

6. **Stage 2: Match overview -- loop until no pending work**
   ```python
   if not shutdown.is_set:
       logger.info("=== Stage 2: Match Overview ===")
   while not shutdown.is_set and not failure_tracker.should_halt:
       stats = await run_match_overview(
           client, match_repo, discovery_repo, storage, config,
       )
       results["overview"]["parsed"] += stats["parsed"]
       results["overview"]["failed"] += stats["failed"]
       if stats["batch_size"] == 0:
           break
       if stats["parsed"] > 0:
           failure_tracker.record_success()
       if stats["fetch_errors"] > 0 or (stats["failed"] > 0 and stats["parsed"] == 0):
           if failure_tracker.record_failure():
               results["halted"] = True
               results["halt_reason"] = "Consecutive failures exceeded threshold (overview stage)"
               break
       progress.log_stage("Match overview batch", stats)
   ```

7. **Stage 3: Map stats -- loop until no pending work** (same pattern as Stage 2)
   ```python
   if not shutdown.is_set and not failure_tracker.should_halt:
       logger.info("=== Stage 3: Map Stats ===")
   while not shutdown.is_set and not failure_tracker.should_halt:
       stats = await run_map_stats(client, match_repo, storage, config)
       results["map_stats"]["parsed"] += stats["parsed"]
       results["map_stats"]["failed"] += stats["failed"]
       if stats["batch_size"] == 0:
           break
       if stats["parsed"] > 0:
           failure_tracker.record_success()
       if stats["fetch_errors"] > 0 or (stats["failed"] > 0 and stats["parsed"] == 0):
           if failure_tracker.record_failure():
               results["halted"] = True
               results["halt_reason"] = "Consecutive failures exceeded threshold (map stats stage)"
               break
       progress.log_stage("Map stats batch", stats)
   ```

8. **Stage 4: Performance + Economy -- loop until no pending work** (same pattern)
   ```python
   if not shutdown.is_set and not failure_tracker.should_halt:
       logger.info("=== Stage 4: Performance + Economy ===")
   while not shutdown.is_set and not failure_tracker.should_halt:
       stats = await run_performance_economy(client, match_repo, storage, config)
       results["perf_economy"]["parsed"] += stats["parsed"]
       results["perf_economy"]["failed"] += stats["failed"]
       if stats["batch_size"] == 0:
           break
       if stats["parsed"] > 0:
           failure_tracker.record_success()
       if stats["fetch_errors"] > 0 or (stats["failed"] > 0 and stats["parsed"] == 0):
           if failure_tracker.record_failure():
               results["halted"] = True
               results["halt_reason"] = "Consecutive failures exceeded threshold (perf/economy stage)"
               break
       progress.log_stage("Perf/economy batch", stats)
   ```

9. **Summary**: Attach progress summary to results.
   ```python
   results["summary"] = progress.summary()
   if shutdown.is_set:
       results["halted"] = True
       results["halt_reason"] = "User requested shutdown (Ctrl+C)"
   return results
   ```

Note: Use untyped parameters (same as existing orchestrators) to avoid circular imports. The pipeline function orchestrates -- it does not import types.
  </action>
  <verify>
`python -c "from scraper.pipeline import run_pipeline; print('OK')"` -- import succeeds without errors.
  </verify>
  <done>run_pipeline calls all 4 orchestrators in sequence with loop-until-done, shutdown checking, and consecutive failure tracking.</done>
</task>

<task type="auto">
  <name>Task 2: CLI entry point</name>
  <files>src/scraper/cli.py</files>
  <action>
Create `src/scraper/cli.py` with an argparse-based CLI entry point.

**`build_parser()` function:**
```python
def build_parser() -> argparse.ArgumentParser:
```
Arguments:
- `--start-offset` (int, default 0): Start offset for results pagination
- `--end-offset` (int, default 9900): End offset for results pagination
- `--full` (store_true): Full scrape -- re-discover all matches in range (disables incremental early termination)
- `--force-rescrape` (store_true): Re-process already-complete matches (resets scraped -> pending)
- `--data-dir` (str, default "data"): Data directory for DB, HTML archive, and logs

Program name: `hltv-scraper`
Description: `"Scrape CS2 match data from HLTV.org"`

**`async_main(args)` async function:**
1. Call `setup_logging(data_dir=args.data_dir)` -- returns log file path.
2. Log the run configuration: offset range, mode (incremental/full), data dir, log file path.
3. Create `ScraperConfig` with:
   - `data_dir=args.data_dir`
   - `db_path=f"{args.data_dir}/hltv.db"`
   - `start_offset=args.start_offset`
   - `max_offset=args.end_offset`
4. Create `ShutdownHandler` and call `shutdown.install()`.
5. Initialize Database, connect, apply migrations.
6. Create MatchRepository, DiscoveryRepository, HtmlStorage.
7. If `--force-rescrape`: reset all scraped matches to pending via `discovery_repo.update_all_to_pending()` -- actually, for force-rescrape we need a new method. Instead, let `run_pipeline` handle the `force_rescrape` flag by doing reset in its logic. For now, just pass the flag through.
8. Open `async with HLTVClient(config) as client:` and call:
   ```python
   results = await run_pipeline(
       client=client,
       match_repo=match_repo,
       discovery_repo=discovery_repo,
       storage=storage,
       config=config,
       shutdown=shutdown,
       incremental=not args.full,
       force_rescrape=args.force_rescrape,
   )
   ```
9. In a `finally` block:
   - Print the end-of-run summary from `results` (use `progress.format_summary()` or format from results dict)
   - Log the summary to the log file too
   - Call `db.close()`
   - Call `shutdown.restore()`
   - Call `logging.shutdown()` to flush log file handlers

**`main()` function (sync entry point):**
```python
def main():
    parser = build_parser()
    args = parser.parse_args()
    try:
        asyncio.run(async_main(args))
    except KeyboardInterrupt:
        pass  # Already handled by ShutdownHandler
```

**End-of-run summary format** (logged at INFO):
```
============================================================
Pipeline complete
------------------------------------------------------------
Discovery:   {discovered} matches found ({new_matches} new)
Overview:    {parsed} parsed, {failed} failed
Map Stats:   {parsed} parsed, {failed} failed
Perf/Econ:   {parsed} parsed, {failed} failed
------------------------------------------------------------
Wall time:   {wall_time:.0f}s
Log file:    {log_file_path}
============================================================
```

If halted, also show: `Halted: {halt_reason}`

Important: The summary must be printed even on Ctrl+C. Use try/finally to guarantee it.
  </action>
  <verify>
`python -c "from scraper.cli import main, build_parser; p = build_parser(); args = p.parse_args([]); print(args)"` -- verify default args parse correctly (start_offset=0, end_offset=9900, full=False, force_rescrape=False, data_dir='data'). Then `python -c "from scraper.cli import main; print('CLI importable')"`.
  </verify>
  <done>CLI parses arguments, sets up logging, initializes all components, runs pipeline, prints summary on exit.</done>
</task>

<task type="auto">
  <name>Task 3: pyproject.toml entry point</name>
  <files>pyproject.toml</files>
  <action>
Add a `[project.scripts]` section to pyproject.toml:

```toml
[project.scripts]
hltv-scraper = "scraper.cli:main"
```

After adding the entry point, run `pip install -e .` to register the console script. Then verify the command is available.
  </action>
  <verify>
`pip install -e "C:/Users/Mads/Desktop/scraper"` succeeds. Then `python -m scraper.cli --help` prints the argparse help text with all expected flags.
  </verify>
  <done>hltv-scraper command is registered and prints help when invoked with --help.</done>
</task>

</tasks>

<verification>
1. `python -c "from scraper.pipeline import run_pipeline"` -- pipeline runner importable
2. `python -c "from scraper.cli import main"` -- CLI importable
3. `python -m scraper.cli --help` -- prints help with all 5 flags
4. `pip install -e .` -- entry point registered
5. No new dependencies added (all stdlib)
</verification>

<success_criteria>
- run_pipeline orchestrates all 4 stages in correct order with loop-until-done
- Pipeline respects shutdown flag between batches
- Pipeline halts on consecutive failures
- CLI accepts --start-offset, --end-offset, --full, --force-rescrape, --data-dir
- End-of-run summary always prints (even on Ctrl+C)
- Browser is always cleaned up (try/finally in async_main)
- hltv-scraper console script registered in pyproject.toml
</success_criteria>

<output>
After completion, create `.planning/phases/09-pipeline-orchestration/09-03-SUMMARY.md`
</output>
