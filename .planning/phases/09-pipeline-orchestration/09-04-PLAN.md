---
phase: 09-pipeline-orchestration
plan: 04
type: execute
wave: 3
depends_on: ["09-03"]
files_modified:
  - tests/test_pipeline.py
  - tests/test_cli.py
autonomous: false

must_haves:
  truths:
    - "Pipeline stages run in correct order and loop until no work remains"
    - "Consecutive failure threshold halts the pipeline"
    - "Shutdown flag causes graceful pipeline exit"
    - "CLI argument parsing produces correct config values"
    - "All existing tests still pass"
  artifacts:
    - path: "tests/test_pipeline.py"
      provides: "Unit tests for pipeline utilities and run_pipeline"
      min_lines: 80
    - path: "tests/test_cli.py"
      provides: "Unit tests for CLI argument parsing"
      min_lines: 30
  key_links:
    - from: "tests/test_pipeline.py"
      to: "src/scraper/pipeline.py"
      via: "imports and tests"
      pattern: "from scraper\\.pipeline import"
    - from: "tests/test_cli.py"
      to: "src/scraper/cli.py"
      via: "imports and tests"
      pattern: "from scraper\\.cli import"
---

<objective>
Write comprehensive unit tests for the pipeline utilities, pipeline runner, and CLI, then run the full test suite to verify no regressions.

Purpose: Phase 9 adds significant orchestration logic. Tests ensure the pipeline runner correctly sequences stages, respects shutdown and failure thresholds, and that the CLI correctly maps arguments to config.
Output: `tests/test_pipeline.py`, `tests/test_cli.py`, full test suite green.
</objective>

<execution_context>
@~/.claude/get-shit-done/workflows/execute-plan.md
@~/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/09-pipeline-orchestration/09-CONTEXT.md
@.planning/phases/09-pipeline-orchestration/09-01-SUMMARY.md
@.planning/phases/09-pipeline-orchestration/09-02-SUMMARY.md
@.planning/phases/09-pipeline-orchestration/09-03-SUMMARY.md
@src/scraper/pipeline.py
@src/scraper/cli.py
@src/scraper/config.py
@tests/test_match_overview.py
@tests/test_map_stats.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Pipeline and CLI unit tests</name>
  <files>tests/test_pipeline.py, tests/test_cli.py</files>
  <action>
**tests/test_pipeline.py** -- test pipeline utilities and runner:

Use the project's established test patterns: pytest, `@pytest.mark.asyncio` on async tests, mocked dependencies (see test_match_overview.py and test_map_stats.py for patterns).

**ConsecutiveFailureTracker tests:**
- `test_tracker_initial_state`: not should_halt, consecutive=0
- `test_tracker_under_threshold`: 2 failures with threshold=3, not should_halt
- `test_tracker_at_threshold`: 3 failures, should_halt=True, record_failure returns True
- `test_tracker_reset_on_success`: 2 failures then success, not should_halt, consecutive=0
- `test_tracker_custom_threshold`: threshold=1, first failure triggers halt

**ProgressTracker tests:**
- `test_progress_initial_state`: completed=0, failed=0, skipped=0
- `test_progress_log_match`: increments completed, verify log output (use caplog fixture)
- `test_progress_summary`: returns dict with correct keys and wall_time > 0
- `test_progress_format_summary`: returns non-empty string

**ShutdownHandler tests:**
- `test_shutdown_initial_state`: is_set is False
- `test_shutdown_set`: after manually setting the internal event, is_set is True

**run_pipeline tests** (mock all orchestrators):

Create mock async functions that return orchestrator-compatible stats dicts. Use `unittest.mock.AsyncMock` or define simple async functions.

- `test_pipeline_runs_all_stages`: Mock all 4 orchestrators to return batch_size=0 on first call (no pending work). Verify all 4 are called. Use `unittest.mock.patch` on the imported functions.

- `test_pipeline_loops_until_no_work`: Mock run_match_overview to return batch_size=5 (with parsed=5, failed=0) on first call, then batch_size=0 on second call. Verify it's called twice.

- `test_pipeline_halts_on_consecutive_failures`: Mock run_match_overview to return batch_size=5 with fetch_errors=1 and parsed=0 on every call. Verify pipeline sets results["halted"]=True.

- `test_pipeline_respects_shutdown`: Create a shutdown handler, set the event, pass to run_pipeline. Verify pipeline returns quickly without calling orchestrators (or calling discovery only briefly before checking).

- `test_pipeline_resets_failed_matches`: Mock discovery_repo.reset_failed_matches. Verify it's called when force_rescrape=False.

For mocking orchestrators in run_pipeline, patch `scraper.pipeline.run_discovery`, `scraper.pipeline.run_match_overview`, etc. (the imported names in pipeline.py).

**tests/test_cli.py** -- test CLI argument parsing:

- `test_default_args`: parse empty argv, verify start_offset=0, end_offset=9900, full=False, force_rescrape=False, data_dir="data"
- `test_custom_offsets`: parse `["--start-offset", "100", "--end-offset", "500"]`
- `test_full_flag`: parse `["--full"]`, verify full=True
- `test_force_rescrape_flag`: parse `["--force-rescrape"]`, verify force_rescrape=True
- `test_custom_data_dir`: parse `["--data-dir", "/tmp/scrape"]`, verify data_dir="/tmp/scrape"
- `test_all_flags_combined`: parse all flags together, verify all values

Use `build_parser().parse_args(argv)` directly for testing -- no need to mock sys.argv.

After writing tests, run the full test suite: `python -m pytest tests/ -m "not integration" -v` to verify all tests pass including the new ones plus no regressions.
  </action>
  <verify>
`python -m pytest tests/test_pipeline.py tests/test_cli.py -v` -- all new tests pass. Then `python -m pytest tests/ -m "not integration" -v` -- full suite passes with no regressions.
  </verify>
  <done>Pipeline utilities, runner, and CLI have comprehensive unit tests. Full test suite passes.</done>
</task>

<task type="checkpoint:human-verify" gate="blocking">
  <what-built>
Complete Phase 9 pipeline orchestration:
- Logging config with console + file output
- Pipeline runner wiring all 4 orchestrators with loop-until-done
- Graceful Ctrl+C shutdown via cross-platform signal handling
- Consecutive failure threshold (halts on systemic issues)
- Incremental discovery (early termination when all matches known)
- CLI entry point with --start-offset, --end-offset, --full, --force-rescrape flags
- End-of-run summary with per-stage counts and timing
- Comprehensive unit tests
  </what-built>
  <how-to-verify>
1. Run `python -m scraper.cli --help` and verify all 5 flags appear with descriptions
2. Run `python -m pytest tests/ -m "not integration" -v` and verify all tests pass (including new pipeline + CLI tests)
3. (Optional quick smoke test) Run `python -m scraper.cli --start-offset 0 --end-offset 0 --data-dir data/test-run` -- should initialize DB, attempt one page of discovery (will fail without Chrome or succeed with Chrome), then print summary. This verifies the full pipeline bootstraps correctly.
4. Verify `data/logs/` contains a timestamped log file after any run
  </how-to-verify>
  <resume-signal>Type "approved" or describe issues to fix</resume-signal>
</task>

</tasks>

<verification>
1. `python -m pytest tests/ -m "not integration" -v` -- full test suite green
2. `python -m scraper.cli --help` -- shows correct help text
3. Pipeline utility classes (ShutdownHandler, ConsecutiveFailureTracker, ProgressTracker) tested
4. Pipeline runner tested with mocked orchestrators
5. CLI argument parsing tested
</verification>

<success_criteria>
- All new tests pass
- All existing tests pass (no regressions)
- Pipeline correctly orchestrates all 4 stages
- Shutdown, failure threshold, and incremental mode tested
- CLI argument defaults and flag combinations tested
- Human verifies --help output and test results
</success_criteria>

<output>
After completion, create `.planning/phases/09-pipeline-orchestration/09-04-SUMMARY.md`
</output>
