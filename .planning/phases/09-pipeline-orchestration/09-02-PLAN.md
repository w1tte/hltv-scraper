---
phase: 09-pipeline-orchestration
plan: 02
type: execute
wave: 1
depends_on: []
files_modified:
  - src/scraper/config.py
  - src/scraper/discovery.py
  - src/scraper/discovery_repository.py
autonomous: true

must_haves:
  truths:
    - "Discovery stops early when an entire page of matches is already in DB"
    - "Pipeline config includes start_offset and end_offset for scoping"
    - "Failed matches are auto-reset to pending at pipeline start"
    - "--full flag makes discovery re-process all pages (no early termination)"
  artifacts:
    - path: "src/scraper/config.py"
      provides: "Pipeline-specific config fields (start_offset, consecutive_failure_threshold)"
      contains: "start_offset"
    - path: "src/scraper/discovery.py"
      provides: "Incremental discovery with early termination"
      exports: ["run_discovery"]
    - path: "src/scraper/discovery_repository.py"
      provides: "count_new_matches and reset_failed_matches methods"
      contains: "count_new_matches"
  key_links:
    - from: "src/scraper/discovery.py"
      to: "src/scraper/discovery_repository.py"
      via: "count_new_matches call for early termination"
      pattern: "count_new_matches"
    - from: "src/scraper/config.py"
      to: "src/scraper/discovery.py"
      via: "start_offset/max_offset fields"
      pattern: "config\\.start_offset|config\\.max_offset"
---

<objective>
Extend the discovery system for incremental mode (early termination when all matches on a page are already known) and add pipeline-specific config fields. Also add a method to auto-reset failed matches to pending state at pipeline start.

Purpose: Incremental mode is the default behavior -- the pipeline skips already-discovered matches and halts discovery early. This is critical for efficient re-runs and new match detection.
Output: Modified config.py, discovery.py, and discovery_repository.py with incremental capabilities.
</objective>

<execution_context>
@~/.claude/get-shit-done/workflows/execute-plan.md
@~/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/09-pipeline-orchestration/09-CONTEXT.md
@.planning/phases/09-pipeline-orchestration/09-RESEARCH.md
@src/scraper/config.py
@src/scraper/discovery.py
@src/scraper/discovery_repository.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Extend ScraperConfig with pipeline fields</name>
  <files>src/scraper/config.py</files>
  <action>
Add the following fields to the ScraperConfig dataclass:

```python
# Pipeline orchestration
start_offset: int = 0              # Start offset for results pagination
consecutive_failure_threshold: int = 3  # Halt pipeline after N consecutive failures
```

The existing `max_offset` field (default 9900) already serves as the end offset. The `start_offset` field is new and defaults to 0. The CLI will map --start-offset and --end-offset to `start_offset` and `max_offset` respectively.

Do NOT rename `max_offset` to `end_offset` -- existing code uses `max_offset` throughout discovery.py and tests. Just add `start_offset` as the new companion field.
  </action>
  <verify>
`python -c "from scraper.config import ScraperConfig; c = ScraperConfig(); assert c.start_offset == 0; assert c.consecutive_failure_threshold == 3; print('OK')"` passes.
  </verify>
  <done>ScraperConfig has start_offset and consecutive_failure_threshold fields with sensible defaults.</done>
</task>

<task type="auto">
  <name>Task 2: Incremental discovery + failed match reset</name>
  <files>src/scraper/discovery.py, src/scraper/discovery_repository.py</files>
  <action>
**In discovery_repository.py**, add two new methods to the DiscoveryRepository class:

1. `count_new_matches(self, match_ids: list[int]) -> int`:
   - Counts how many of the given match_ids are NOT already in `scrape_queue`.
   - SQL: `SELECT COUNT(*) FROM scrape_queue WHERE match_id IN ({placeholders})` then subtract from `len(match_ids)`.
   - Returns the count of truly new (not yet in DB) matches.
   - Handle empty list edge case (return 0).

2. `reset_failed_matches(self) -> int`:
   - Updates all `scrape_queue` rows with `status = 'failed'` back to `status = 'pending'`.
   - SQL: `UPDATE scrape_queue SET status = 'pending' WHERE status = 'failed'`
   - Returns the number of rows affected (via `cursor.rowcount`).
   - Uses `with self.conn:` for transaction.

**In discovery.py**, modify `run_discovery` to support incremental mode:

1. Change the function signature to accept an optional `incremental: bool = True` parameter and an optional `shutdown` parameter (an object with `.is_set` property, or None):
   ```python
   async def run_discovery(
       client, repo, storage, config,
       incremental: bool = True,
       shutdown=None,
   ) -> dict:
   ```

2. Change the range to use `config.start_offset` instead of hardcoded 0:
   ```python
   for offset in range(config.start_offset, config.max_offset + 1, config.results_per_page):
   ```

3. Add shutdown check at the start of each loop iteration:
   ```python
   if shutdown is not None and shutdown.is_set:
       logger.info("Shutdown requested, stopping discovery at offset %d", offset)
       break
   ```

4. After calling `repo.persist_page(batch, offset)`, add incremental early termination:
   ```python
   if incremental:
       new_count = repo.count_new_matches([m.match_id for m in matches])
       if new_count == 0:
           logger.info(
               "All %d matches on offset %d already known. "
               "Stopping incremental discovery.",
               len(matches), offset,
           )
           break
   ```

   Note: `count_new_matches` must be called AFTER `persist_page` because persist_page does UPSERT. To count truly new matches, call `count_new_matches` BEFORE `persist_page` instead. Reorder: check new count first, then persist. This way persist_page's UPSERT doesn't affect the count.

   Corrected order in the loop body:
   ```python
   # Parse
   matches = parse_results_page(html)
   # ... validate count ...

   # Check for incremental early termination BEFORE persisting
   if incremental:
       new_count = repo.count_new_matches([m.match_id for m in matches])
       stats["new_matches"] += new_count
       if new_count == 0:
           logger.info("All %d matches on offset %d already known. Stopping.", len(matches), offset)
           break

   # Persist batch
   repo.persist_page(batch, offset)
   stats["matches_found"] += len(matches)
   ```

5. Add `"new_matches": 0` to the stats dict. In non-incremental mode, `new_matches` is not tracked (set equal to `matches_found` at end).

Important: The existing `run_discovery` function signature uses untyped parameters to avoid circular imports. Keep that pattern. The `shutdown` parameter should accept any object with an `is_set` property (duck typing), not specifically `asyncio.Event` or `ShutdownHandler`.
  </action>
  <verify>
Run existing discovery tests to ensure no regressions: `python -m pytest tests/test_discovery_repository.py -v`. Then verify new method exists: `python -c "from scraper.discovery_repository import DiscoveryRepository; print('count_new_matches' in dir(DiscoveryRepository)); print('reset_failed_matches' in dir(DiscoveryRepository))"` -- both should print True.
  </verify>
  <done>Discovery supports incremental mode with early termination, config has start_offset, failed matches can be auto-reset to pending.</done>
</task>

</tasks>

<verification>
1. `python -m pytest tests/test_discovery_repository.py -v` -- existing tests pass (no regressions)
2. `python -c "from scraper.config import ScraperConfig; c = ScraperConfig(start_offset=100); assert c.start_offset == 100"` -- config extension works
3. `python -c "from scraper.discovery_repository import DiscoveryRepository; assert hasattr(DiscoveryRepository, 'count_new_matches')"` -- new method exists
4. `python -c "from scraper.discovery_repository import DiscoveryRepository; assert hasattr(DiscoveryRepository, 'reset_failed_matches')"` -- new method exists
</verification>

<success_criteria>
- ScraperConfig has start_offset and consecutive_failure_threshold with correct defaults
- run_discovery respects config.start_offset, shutdown flag, and incremental early termination
- count_new_matches returns accurate count of matches not yet in DB
- reset_failed_matches updates failed->pending and returns affected count
- All existing discovery tests pass without modification
</success_criteria>

<output>
After completion, create `.planning/phases/09-pipeline-orchestration/09-02-SUMMARY.md`
</output>
