---
phase: 09-pipeline-orchestration
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - src/scraper/logging_config.py
  - src/scraper/pipeline.py
autonomous: true

must_haves:
  truths:
    - "Pipeline logs to both console and a timestamped file"
    - "Ctrl+C sets a shutdown flag instead of crashing"
    - "Consecutive failures are tracked across the pipeline"
    - "Per-match progress is logged with timing"
  artifacts:
    - path: "src/scraper/logging_config.py"
      provides: "Console + file logging setup"
      exports: ["setup_logging"]
    - path: "src/scraper/pipeline.py"
      provides: "ShutdownHandler, ConsecutiveFailureTracker, ProgressTracker"
      min_lines: 80
  key_links:
    - from: "src/scraper/logging_config.py"
      to: "logging module"
      via: "root logger configuration"
      pattern: "logging\\.getLogger|addHandler"
    - from: "src/scraper/pipeline.py"
      to: "signal module"
      via: "SIGINT handler"
      pattern: "signal\\.signal.*SIGINT"
---

<objective>
Create the foundational utilities for the pipeline: structured logging configuration and three utility classes (ShutdownHandler for graceful Ctrl+C, ConsecutiveFailureTracker for halt-on-systemic-failure, ProgressTracker for per-match progress logging with timing and end-of-run summary).

Purpose: These utilities are prerequisites for the pipeline runner (Plan 03) and must exist before the pipeline can be wired together.
Output: Two new modules -- `logging_config.py` and `pipeline.py` (utilities only, no pipeline runner function yet).
</objective>

<execution_context>
@~/.claude/get-shit-done/workflows/execute-plan.md
@~/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/09-pipeline-orchestration/09-CONTEXT.md
@.planning/phases/09-pipeline-orchestration/09-RESEARCH.md
@src/scraper/config.py
@src/scraper/http_client.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Logging configuration module</name>
  <files>src/scraper/logging_config.py</files>
  <action>
Create `src/scraper/logging_config.py` with a `setup_logging(data_dir: str = "data", console_level: int = logging.INFO) -> Path` function.

Implementation details:
- Create `{data_dir}/logs/` directory (mkdir parents=True, exist_ok=True)
- Generate timestamped log filename: `run-YYYY-MM-DD-HHMMSS.log`
- Configure the root logger at DEBUG level
- Add a StreamHandler for console output:
  - Level: `console_level` parameter (default INFO)
  - Format: `"%(asctime)s %(levelname)-5s %(message)s"` with datefmt `"%H:%M:%S"`
- Add a FileHandler for the log file:
  - Level: DEBUG (captures everything)
  - Format: `"%(asctime)s %(levelname)-5s [%(name)s] %(message)s"` (full datetime, includes logger name)
  - Encoding: utf-8
- Suppress noisy third-party loggers: set `nodriver` and `uc` loggers to WARNING level
- Return the Path to the log file

Important: Clear existing handlers on root logger before adding new ones (call `root.handlers.clear()`) to prevent duplicate handlers if setup_logging is called multiple times (e.g., in tests).
  </action>
  <verify>
`python -c "from scraper.logging_config import setup_logging; p = setup_logging(); print(f'Log file: {p}')"` creates the log file and prints its path. Verify `data/logs/run-*.log` exists.
  </verify>
  <done>setup_logging creates console + file handlers, returns log file path, suppresses noisy loggers.</done>
</task>

<task type="auto">
  <name>Task 2: Pipeline utility classes (ShutdownHandler, ConsecutiveFailureTracker, ProgressTracker)</name>
  <files>src/scraper/pipeline.py</files>
  <action>
Create `src/scraper/pipeline.py` with three utility classes. The pipeline runner function will be added in Plan 03.

**ShutdownHandler:**
- `__init__(self)`: Creates an `asyncio.Event` internally, stores `_original_handler = None`
- `install(self)`: Saves the current SIGINT handler via `signal.getsignal(signal.SIGINT)`, then installs a custom handler via `signal.signal(signal.SIGINT, self._handle)`. Must work on Windows (do NOT use `loop.add_signal_handler`).
- `_handle(self, sig, frame)`: On first Ctrl+C, logs "Shutdown requested. Finishing current work..." and sets the event. On second Ctrl+C (event already set), logs "Force shutdown" and raises `SystemExit(1)`.
- `is_set` property: Returns `self._event.is_set()`
- `restore(self)`: Restores original signal handler if saved.

**ConsecutiveFailureTracker:**
- `__init__(self, threshold: int = 3)`: Stores threshold, initializes `self.consecutive = 0`
- `record_success(self)`: Resets `self.consecutive = 0`
- `record_failure(self) -> bool`: Increments `self.consecutive`, returns True if `>= threshold`
- `should_halt` property: Returns `self.consecutive >= self.threshold`

**ProgressTracker:**
- `__init__(self, total: int = 0)`: Stores total, initializes completed/failed/skipped to 0, records start time via `time.monotonic()`
- `log_match(self, match_id: int, status: str, elapsed: float)`: Increments completed. Logs at INFO: `"[{completed}/{total}] match {match_id} {symbol} ({elapsed:.1f}s)"` where symbol is "ok" for status=="scraped" and "FAIL" otherwise. If total is 0, format as `"[{completed}]"` without total.
- `log_stage(self, stage: str, stats: dict)`: Logs a summary line for a completed stage (e.g., "Discovery: 500 matches found" or "Match overview: 45 parsed, 2 failed")
- `summary(self) -> dict`: Returns dict with `completed`, `failed`, `skipped`, `wall_time` (monotonic delta). Used by the CLI for the end-of-run summary.
- `format_summary(self) -> str`: Returns a human-readable multiline summary string with counts and wall time.

All classes use `logging.getLogger(__name__)` for their logger. Import `asyncio`, `signal`, `time`, `logging` from stdlib only.
  </action>
  <verify>
`python -c "from scraper.pipeline import ShutdownHandler, ConsecutiveFailureTracker, ProgressTracker; print('All classes imported')"` succeeds. Then verify ConsecutiveFailureTracker: `python -c "from scraper.pipeline import ConsecutiveFailureTracker; t = ConsecutiveFailureTracker(3); assert not t.should_halt; t.record_failure(); t.record_failure(); assert not t.should_halt; assert t.record_failure(); assert t.should_halt; t.record_success(); assert not t.should_halt; print('OK')"`.
  </verify>
  <done>ShutdownHandler installs cross-platform SIGINT handler, ConsecutiveFailureTracker tracks consecutive failures with configurable threshold, ProgressTracker logs per-match progress with timing.</done>
</task>

</tasks>

<verification>
1. `python -c "from scraper.logging_config import setup_logging"` -- no import error
2. `python -c "from scraper.pipeline import ShutdownHandler, ConsecutiveFailureTracker, ProgressTracker"` -- no import error
3. `data/logs/` directory created after setup_logging call
4. ConsecutiveFailureTracker threshold logic works (verified via inline test above)
</verification>

<success_criteria>
- logging_config.py creates dual console+file logging with correct formats
- ShutdownHandler uses signal.signal (not loop.add_signal_handler) for Windows compatibility
- ConsecutiveFailureTracker halts at threshold, resets on success
- ProgressTracker logs formatted progress lines with timing
- All imports succeed with no new dependencies
</success_criteria>

<output>
After completion, create `.planning/phases/09-pipeline-orchestration/09-01-SUMMARY.md`
</output>
