---
phase: 04-match-discovery
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - migrations/002_scrape_queue.sql
  - src/scraper/config.py
  - src/scraper/storage.py
  - src/scraper/discovery_repository.py
  - tests/test_discovery_repository.py
  - tests/test_storage_results.py
autonomous: true

must_haves:
  truths:
    - "scrape_queue table exists and accepts match discovery rows via UPSERT"
    - "discovery_progress table tracks which offsets have been processed"
    - "UPSERT on re-discovery does NOT reset status from 'scraped' back to 'pending'"
    - "Results page HTML can be saved/loaded by offset (not match ID)"
    - "ScraperConfig has max_offset and results_per_page fields"
  artifacts:
    - path: "migrations/002_scrape_queue.sql"
      provides: "scrape_queue and discovery_progress table definitions"
      contains: "CREATE TABLE IF NOT EXISTS scrape_queue"
    - path: "src/scraper/discovery_repository.py"
      provides: "DiscoveryRepository with UPSERT queue operations"
      exports: ["DiscoveryRepository"]
    - path: "src/scraper/storage.py"
      provides: "Extended HtmlStorage with results page methods"
      contains: "save_results_page"
    - path: "src/scraper/config.py"
      provides: "ScraperConfig with discovery pagination fields"
      contains: "max_offset"
    - path: "tests/test_discovery_repository.py"
      provides: "Unit tests for DiscoveryRepository"
      min_lines: 80
    - path: "tests/test_storage_results.py"
      provides: "Unit tests for HtmlStorage results page methods"
      min_lines: 30
  key_links:
    - from: "src/scraper/discovery_repository.py"
      to: "migrations/002_scrape_queue.sql"
      via: "SQL references scrape_queue and discovery_progress tables"
      pattern: "scrape_queue"
    - from: "tests/test_discovery_repository.py"
      to: "src/scraper/discovery_repository.py"
      via: "imports DiscoveryRepository"
      pattern: "from scraper.discovery_repository import"
---

<objective>
Create the database schema, configuration, storage, and repository infrastructure needed for match discovery.

Purpose: Phase 4 needs a scrape_queue table for discovered matches, offset-based progress tracking for resume support, raw HTML archival for results pages, and a repository layer following the established MatchRepository pattern. All of these are prerequisites for the parser (Plan 02) and runner (Plan 03).

Output: Migration SQL, extended config/storage, new DiscoveryRepository, and unit tests for all components.
</objective>

<execution_context>
@~/.claude/get-shit-done/workflows/execute-plan.md
@~/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/04-match-discovery/04-CONTEXT.md
@.planning/phases/04-match-discovery/04-RESEARCH.md

Key source files to reference:
@src/scraper/config.py
@src/scraper/storage.py
@src/scraper/repository.py (MatchRepository pattern to follow)
@src/scraper/db.py (Database.apply_migrations pattern)
@migrations/001_initial_schema.sql (existing schema)
@tests/test_repository.py (test pattern to follow)
@tests/test_storage.py (test pattern to follow)
@tests/test_db.py (fixture pattern: Database(tmp_path) + initialize())
</context>

<tasks>

<task type="auto">
  <name>Task 1: Migration, config extension, and HtmlStorage results page methods</name>
  <files>
    migrations/002_scrape_queue.sql
    src/scraper/config.py
    src/scraper/storage.py
    tests/test_storage_results.py
  </files>
  <action>
    **1. Create migration `migrations/002_scrape_queue.sql`:**

    ```sql
    CREATE TABLE IF NOT EXISTS scrape_queue (
        match_id      INTEGER PRIMARY KEY,
        url           TEXT NOT NULL,
        offset        INTEGER NOT NULL,
        discovered_at TEXT NOT NULL,
        is_forfeit    INTEGER NOT NULL DEFAULT 0,
        status        TEXT NOT NULL DEFAULT 'pending'
    );

    CREATE TABLE IF NOT EXISTS discovery_progress (
        offset       INTEGER PRIMARY KEY,
        completed_at TEXT NOT NULL
    );

    CREATE INDEX IF NOT EXISTS idx_scrape_queue_status ON scrape_queue(status);
    CREATE INDEX IF NOT EXISTS idx_scrape_queue_offset ON scrape_queue(offset);
    ```

    **2. Extend `src/scraper/config.py`** -- add two fields to ScraperConfig dataclass:
    - `max_offset: int = 9900` (last offset to paginate to, inclusive)
    - `results_per_page: int = 100` (entries per results page, HLTV constant)

    Place these after the existing `db_path` field with a comment `# Discovery pagination`.

    **3. Extend `src/scraper/storage.py`** -- add three new methods to HtmlStorage class:

    - `save_results_page(self, html: str, offset: int) -> Path`: Save to `self.base_dir / "results" / f"offset-{offset}.html.gz"`. Create parent dirs. Gzip compress. Return path.
    - `load_results_page(self, offset: int) -> str`: Load and decompress from same path. Raise `FileNotFoundError` with descriptive message if file missing.
    - `results_page_exists(self, offset: int) -> bool`: Return whether the file exists.

    These methods are SEPARATE from the existing `save`/`load`/`exists` methods (which use match_id + page_type). Results pages use offset-based paths under `results/` not `matches/`. Do NOT modify PAGE_TYPES dict or the existing `_build_path` method.

    **4. Create `tests/test_storage_results.py`** -- unit tests for the new methods:
    - `test_save_and_load_results_page`: Round-trip save/load with offset=0
    - `test_save_and_load_results_page_different_offsets`: Save offset 0 and 100, load both, verify correct content
    - `test_results_page_exists_true`: After save, exists returns True
    - `test_results_page_exists_false`: Before save, exists returns False
    - `test_load_results_page_missing_raises`: Load non-existent offset raises FileNotFoundError
    - `test_results_page_path_structure`: Verify saved file is at `base_dir/results/offset-{N}.html.gz`
    - `test_results_page_gzip_compressed`: Verify file starts with gzip magic bytes (0x1f, 0x8b)

    Follow the same pattern as `tests/test_storage.py` (use `tmp_path` fixture, create HtmlStorage(tmp_path)).
  </action>
  <verify>
    Run `python -m pytest tests/test_storage_results.py -v` -- all tests pass.
    Run `python -m pytest tests/test_db.py -v` -- existing tests still pass (migration is picked up by apply_migrations).
    Verify the migration file increments schema version to 2 by checking that after `db.initialize()`, `db.get_schema_version() == 2`.
  </verify>
  <done>
    - 002_scrape_queue.sql creates scrape_queue and discovery_progress tables with indexes
    - ScraperConfig has max_offset=9900 and results_per_page=100
    - HtmlStorage has save_results_page, load_results_page, results_page_exists methods
    - All new and existing storage tests pass
    - Schema version is 2 after migrations applied
  </done>
</task>

<task type="auto">
  <name>Task 2: DiscoveryRepository with UPSERT semantics and unit tests</name>
  <files>
    src/scraper/discovery_repository.py
    tests/test_discovery_repository.py
  </files>
  <action>
    **1. Create `src/scraper/discovery_repository.py`** following the MatchRepository pattern exactly:

    Module-level SQL constants:
    ```python
    UPSERT_QUEUE = """
        INSERT INTO scrape_queue (match_id, url, offset, discovered_at, is_forfeit, status)
        VALUES (:match_id, :url, :offset, :discovered_at, :is_forfeit, 'pending')
        ON CONFLICT(match_id) DO UPDATE SET
            url           = excluded.url,
            offset        = excluded.offset,
            discovered_at = excluded.discovered_at,
            is_forfeit    = excluded.is_forfeit
    """
    ```

    CRITICAL: The ON CONFLICT clause must NOT update `status`. On re-discovery, an already-scraped match keeps its status. Only url, offset, discovered_at, is_forfeit are refreshed.

    Class `DiscoveryRepository`:
    - `__init__(self, conn: sqlite3.Connection)` -- takes raw connection (not Database), matching MatchRepository pattern
    - `upsert_batch(self, batch: list[dict]) -> None` -- atomically upsert a page of matches. Use `with self.conn:` wrapping a loop of `self.conn.execute(UPSERT_QUEUE, row)` (same pattern as MatchRepository batch methods).
    - `mark_offset_complete(self, offset: int) -> None` -- INSERT OR REPLACE into discovery_progress with current UTC timestamp. Use `with self.conn:` for auto-commit.
    - `get_completed_offsets(self) -> set[int]` -- SELECT all offsets from discovery_progress, return as set of ints.
    - `count_pending(self) -> int` -- SELECT COUNT(*) FROM scrape_queue WHERE status = 'pending'
    - `count_total(self) -> int` -- SELECT COUNT(*) FROM scrape_queue
    - `get_queue_entry(self, match_id: int) -> dict | None` -- SELECT * WHERE match_id = ?, return dict(row) or None. (For testing and future use.)

    Also add a combined method:
    - `persist_page(self, batch: list[dict], offset: int) -> None` -- atomically upsert the batch AND mark the offset complete in a single `with self.conn:` block. This ensures both operations succeed or both roll back.

    **2. Create `tests/test_discovery_repository.py`** following test_repository.py patterns:

    Fixtures:
    - `db` fixture: `Database(tmp_path / "test.db")`, call `db.initialize()`, yield, close. (Same as test_repository.py but schema version will be 2 now.)
    - `repo` fixture: `DiscoveryRepository(db.conn)`

    Data helper:
    - `make_queue_entry(match_id=1, **overrides)` returning dict with all required fields (match_id, url, offset, discovered_at, is_forfeit).

    Test classes:

    **TestUpsertBatch:**
    - `test_upsert_batch_inserts`: Insert batch of 3 entries, count_total returns 3
    - `test_upsert_batch_with_forfeit`: Insert entry with is_forfeit=True (use integer 1), verify via get_queue_entry
    - `test_upsert_batch_preserves_status`: Insert entry, manually UPDATE status to 'scraped', re-upsert same match_id with different url. Verify status is still 'scraped' but url is updated. THIS IS THE CRITICAL UPSERT SEMANTIC TEST.
    - `test_upsert_batch_updates_on_conflict`: Upsert entry twice with different offset, verify only 1 row exists and offset is updated

    **TestOffsetProgress:**
    - `test_mark_offset_complete`: Mark offset 0 complete, get_completed_offsets returns {0}
    - `test_mark_multiple_offsets`: Mark 0, 100, 200 complete, get_completed_offsets returns {0, 100, 200}
    - `test_get_completed_offsets_empty`: Initially returns empty set

    **TestPersistPage:**
    - `test_persist_page_atomic`: Call persist_page with 3-entry batch and offset=100. Verify count_total==3 AND 100 in get_completed_offsets.

    **TestCountMethods:**
    - `test_count_pending_and_total`: Insert 3 entries (all pending), count_pending==3, count_total==3
    - `test_count_pending_excludes_scraped`: Insert 3 entries, UPDATE 1 to 'scraped', count_pending==2, count_total==3

    **TestGetQueueEntry:**
    - `test_get_queue_entry_found`: Insert and retrieve, verify all fields
    - `test_get_queue_entry_not_found`: Returns None for non-existent match_id
  </action>
  <verify>
    Run `python -m pytest tests/test_discovery_repository.py -v` -- all tests pass.
    Run `python -m pytest tests/ -m "not integration" -v` -- all existing tests still pass.
  </verify>
  <done>
    - DiscoveryRepository follows MatchRepository pattern (raw connection, module-level SQL, with conn: transactions)
    - UPSERT does NOT clobber status on re-discovery (verified by test)
    - persist_page atomically upserts batch + marks offset complete
    - All count/read methods work correctly
    - Full test suite passes (existing + new)
  </done>
</task>

</tasks>

<verification>
1. `python -m pytest tests/test_storage_results.py tests/test_discovery_repository.py -v` -- all new tests pass
2. `python -m pytest tests/ -m "not integration" -v` -- full unit test suite passes (no regressions)
3. Verify schema version: create a Database, initialize(), check `get_schema_version() == 2`
4. Verify scrape_queue and discovery_progress tables exist with correct columns
5. Verify UPSERT does not reset status (the persist_status test)
</verification>

<success_criteria>
- migrations/002_scrape_queue.sql exists and creates 2 tables + 2 indexes
- ScraperConfig.max_offset == 9900 and ScraperConfig.results_per_page == 100
- HtmlStorage.save_results_page() / load_results_page() / results_page_exists() work correctly
- DiscoveryRepository.upsert_batch() inserts new rows and does NOT clobber status on conflict
- DiscoveryRepository.persist_page() atomically handles batch + offset in one transaction
- All unit tests pass (both new and existing)
</success_criteria>

<output>
After completion, create `.planning/phases/04-match-discovery/04-01-SUMMARY.md`
</output>
