---
phase: 04-match-discovery
plan: 03
type: execute
wave: 2
depends_on: ["04-01", "04-02"]
files_modified:
  - src/scraper/discovery.py
  - tests/test_discovery_integration.py
autonomous: true

must_haves:
  truths:
    - "DiscoveryRunner fetches a results page, archives raw HTML, parses entries, and persists to scrape_queue"
    - "Runner skips already-completed offsets on resume (reads discovery_progress)"
    - "Runner warns on entry count != 100 but only fails on 0 entries"
    - "Runner commits per-page (batch upsert + offset progress in one transaction)"
    - "Full loop paginates from offset 0 to max_offset in steps of 100"
  artifacts:
    - path: "src/scraper/discovery.py"
      provides: "DiscoveryRunner async orchestrator added to existing discovery module"
      contains: "run_discovery"
      min_lines: 80
    - path: "tests/test_discovery_integration.py"
      provides: "Integration test fetching 1-2 live HLTV results pages"
      min_lines: 40
  key_links:
    - from: "src/scraper/discovery.py"
      to: "src/scraper/http_client.py"
      via: "HLTVClient.fetch() for page retrieval"
      pattern: "client\\.fetch"
    - from: "src/scraper/discovery.py"
      to: "src/scraper/discovery_repository.py"
      via: "DiscoveryRepository.persist_page() for atomic persistence"
      pattern: "repo\\.persist_page"
    - from: "src/scraper/discovery.py"
      to: "src/scraper/storage.py"
      via: "HtmlStorage.save_results_page() for raw HTML archival"
      pattern: "storage\\.save_results_page"
    - from: "src/scraper/discovery.py"
      to: "src/scraper/discovery.py"
      via: "parse_results_page() for HTML extraction"
      pattern: "parse_results_page"
---

<objective>
Build the DiscoveryRunner async orchestrator that ties together fetch, archive, parse, and persist into a paginated discovery loop with resume support.

Purpose: This is the main entry point for Phase 4 discovery. It drives the 100-page pagination loop, using HLTVClient for fetching, HtmlStorage for archival, parse_results_page for extraction, and DiscoveryRepository for persistence. Resume support via discovery_progress table enables crash recovery.

Output: run_discovery() async function added to discovery.py, plus an integration test verifying 1-2 live pages.
</objective>

<execution_context>
@~/.claude/get-shit-done/workflows/execute-plan.md
@~/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/04-match-discovery/04-CONTEXT.md
@.planning/phases/04-match-discovery/04-RESEARCH.md

Key source files:
@src/scraper/http_client.py (HLTVClient.fetch() -- async, returns HTML string)
@src/scraper/config.py (ScraperConfig with base_url, max_offset, results_per_page)
@src/scraper/storage.py (HtmlStorage.save_results_page)
@src/scraper/discovery_repository.py (DiscoveryRepository.persist_page, get_completed_offsets)
@src/scraper/discovery.py (DiscoveredMatch, parse_results_page -- from Plan 02)
@tests/test_integration.py (integration test pattern -- pytestmark, @pytest.mark.asyncio)

Prior plan outputs (from Wave 1):
@.planning/phases/04-match-discovery/04-01-SUMMARY.md
@.planning/phases/04-match-discovery/04-02-SUMMARY.md
</context>

<tasks>

<task type="auto">
  <name>Task 1: DiscoveryRunner async orchestrator</name>
  <files>src/scraper/discovery.py</files>
  <action>
    Add the `run_discovery` async function to the EXISTING `src/scraper/discovery.py` file (which already contains DiscoveredMatch and parse_results_page from Plan 02). Do NOT overwrite the existing content -- append to it.

    Add these imports at the top of the file (merge with existing imports):
    ```python
    from datetime import datetime, timezone
    ```

    Add the `run_discovery` function:

    ```python
    async def run_discovery(
        client,           # HLTVClient -- not type-hinted to avoid circular import
        repo,             # DiscoveryRepository
        storage,          # HtmlStorage
        config,           # ScraperConfig
    ) -> dict:
        """Paginate HLTV results pages and populate the scrape_queue.

        For each offset from 0 to config.max_offset (step 100):
        1. Skip if offset already in discovery_progress (resume support)
        2. Fetch the results page via client.fetch()
        3. Archive raw HTML via storage.save_results_page()
        4. Parse entries via parse_results_page()
        5. Persist batch + mark offset complete via repo.persist_page()

        Args:
            client: HLTVClient instance (must be started).
            repo: DiscoveryRepository instance.
            storage: HtmlStorage instance.
            config: ScraperConfig instance.

        Returns:
            Dict with stats: pages_fetched, pages_skipped, matches_found, errors.
        """
        completed = repo.get_completed_offsets()
        stats = {
            "pages_fetched": 0,
            "pages_skipped": 0,
            "matches_found": 0,
            "errors": 0,
        }

        for offset in range(0, config.max_offset + 1, config.results_per_page):
            if offset in completed:
                stats["pages_skipped"] += 1
                logger.debug("Skipping offset %d (already complete)", offset)
                continue

            try:
                # 1. Fetch
                url = f"{config.base_url}/results?offset={offset}"
                html = await client.fetch(url)

                # 2. Archive raw HTML
                storage.save_results_page(html, offset=offset)

                # 3. Parse
                matches = parse_results_page(html)

                # 4. Validate entry count
                if len(matches) == 0:
                    logger.error(
                        "Offset %d: 0 entries found (possible Cloudflare issue). "
                        "Stopping pagination.",
                        offset,
                    )
                    stats["errors"] += 1
                    raise RuntimeError(
                        f"Zero entries found at offset {offset}. "
                        "Likely Cloudflare interstitial or page structure change."
                    )

                if len(matches) != config.results_per_page:
                    logger.warning(
                        "Offset %d: expected %d entries, got %d",
                        offset, config.results_per_page, len(matches),
                    )

                # 5. Persist batch + mark offset complete (atomic)
                now = datetime.now(timezone.utc).isoformat()
                batch = [
                    {
                        "match_id": m.match_id,
                        "url": m.url,
                        "offset": offset,
                        "discovered_at": now,
                        "is_forfeit": int(m.is_forfeit),
                    }
                    for m in matches
                ]
                repo.persist_page(batch, offset)

                stats["pages_fetched"] += 1
                stats["matches_found"] += len(matches)
                logger.info(
                    "Offset %d: %d matches (total: %d)",
                    offset, len(matches), stats["matches_found"],
                )

            except RuntimeError:
                raise  # Re-raise the zero-entries error
            except Exception as exc:
                logger.error("Offset %d failed: %s", offset, exc)
                stats["errors"] += 1
                raise  # Let caller decide retry policy

        logger.info(
            "Discovery complete: %d pages fetched, %d skipped, %d matches found",
            stats["pages_fetched"], stats["pages_skipped"], stats["matches_found"],
        )
        return stats
    ```

    Key design decisions:
    - Function accepts generic types (not type-hinted imports) to avoid circular imports between discovery.py and http_client.py
    - is_forfeit is converted to int (SQLite boolean convention) in the batch dict
    - Raises RuntimeError on 0 entries (Cloudflare issue) -- this halts pagination
    - Raises on any fetch error -- caller (Phase 9 orchestrator) handles retries at the page level
    - Warns but continues on != 100 entries (could be last page or structural variation)
    - Per-page commit via repo.persist_page() ensures resume works
  </action>
  <verify>
    Run `python -c "from scraper.discovery import run_discovery, parse_results_page, DiscoveredMatch; print('All imports OK')"` -- succeeds.
    Verify run_discovery function exists and is async (inspect).
  </verify>
  <done>
    - run_discovery() is an async function in discovery.py alongside the existing parser code
    - Paginates from 0 to max_offset in steps of results_per_page
    - Skips completed offsets for resume support
    - Archives raw HTML before parsing
    - Warns on != 100 entries, raises on 0 entries
    - Atomic per-page persistence via repo.persist_page()
    - Returns stats dict
  </done>
</task>

<task type="auto">
  <name>Task 2: Integration test for live discovery</name>
  <files>tests/test_discovery_integration.py</files>
  <action>
    Create `tests/test_discovery_integration.py` following the pattern in `tests/test_integration.py`.

    This test fetches 1-2 LIVE HLTV results pages and verifies the full pipeline: fetch -> archive -> parse -> persist.

    ```python
    """Live integration test for match discovery.

    Fetches 1-2 real HLTV results pages, parses entries, and persists
    to a temporary database. Requires Chrome + network access.

        python -m pytest tests/test_discovery_integration.py -v -s -m integration --timeout=300
    """

    import pytest

    from scraper.config import ScraperConfig
    from scraper.db import Database
    from scraper.discovery import run_discovery
    from scraper.discovery_repository import DiscoveryRepository
    from scraper.http_client import HLTVClient
    from scraper.storage import HtmlStorage

    pytestmark = pytest.mark.integration
    ```

    **Test:**
    ```python
    @pytest.mark.asyncio
    async def test_discover_first_two_pages(tmp_path):
        """Fetch offset 0 and 100, verify matches persisted and HTML archived."""
        config = ScraperConfig(
            max_offset=100,  # Only 2 pages (offset 0 and 100)
            data_dir=str(tmp_path),
            db_path=str(tmp_path / "test.db"),
        )

        db = Database(config.db_path)
        db.initialize()

        storage = HtmlStorage(tmp_path / "raw")
        repo = DiscoveryRepository(db.conn)

        try:
            async with HLTVClient(config) as client:
                stats = await run_discovery(client, repo, storage, config)

            # Verify stats
            assert stats["pages_fetched"] == 2
            assert stats["pages_skipped"] == 0
            assert stats["matches_found"] == 200  # 100 per page
            assert stats["errors"] == 0

            # Verify database
            assert repo.count_total() == 200
            assert repo.count_pending() == 200

            # Verify progress tracking
            completed = repo.get_completed_offsets()
            assert completed == {0, 100}

            # Verify HTML archival
            assert storage.results_page_exists(0)
            assert storage.results_page_exists(100)

            # Verify resume: run again, should skip both pages
            async with HLTVClient(config) as client2:
                stats2 = await run_discovery(client2, repo, storage, config)

            assert stats2["pages_fetched"] == 0
            assert stats2["pages_skipped"] == 2
            assert stats2["matches_found"] == 0

        finally:
            db.close()
    ```

    Notes:
    - Uses `max_offset=100` to only fetch 2 pages (offsets 0 and 100), keeping test fast (~30-60s with rate limiting)
    - Verifies the full stack: HLTVClient -> HtmlStorage -> parse_results_page -> DiscoveryRepository
    - Verifies resume by running a second time and checking pages_skipped
    - The 200 match count could be slightly less if either page has fewer than 100 entries (unlikely but possible). If this causes flakiness, loosen the assertion to `assert stats["matches_found"] >= 180`
    - Uses `@pytest.mark.asyncio` as required by pytest-asyncio strict mode
    - Marked with `integration` so it's excluded from normal test runs
  </action>
  <verify>
    Run `python -m pytest tests/test_discovery_integration.py -v -s -m integration --timeout=300` -- test passes.
    Verify 200 matches discovered across 2 pages.
    Verify resume works (second run skips both pages).
  </verify>
  <done>
    - Integration test fetches 2 live HLTV results pages
    - Verifies full pipeline: fetch -> archive -> parse -> persist
    - Verifies resume support (second run skips completed pages)
    - Test passes with real Chrome and network access
  </done>
</task>

</tasks>

<verification>
1. `python -m pytest tests/ -m "not integration" -v` -- all unit tests pass (no regressions)
2. `python -m pytest tests/test_discovery_integration.py -v -s -m integration --timeout=300` -- integration test passes
3. `python -c "from scraper.discovery import run_discovery, parse_results_page, DiscoveredMatch; print('OK')"` -- all imports work
4. Verify 200 matches in scrape_queue after integration test
5. Verify raw HTML files exist at tmp_path/raw/results/offset-0.html.gz and offset-100.html.gz
6. Verify second run of run_discovery skips all completed pages
</verification>

<success_criteria>
- run_discovery() paginates, fetches, archives, parses, and persists per page
- Resume support works (completed offsets are skipped)
- Zero-entry pages raise RuntimeError (Cloudflare detection)
- Non-100 entry counts produce warnings but continue
- Integration test proves full pipeline works against live HLTV
- All unit and integration tests pass
</success_criteria>

<output>
After completion, create `.planning/phases/04-match-discovery/04-03-SUMMARY.md`
</output>
