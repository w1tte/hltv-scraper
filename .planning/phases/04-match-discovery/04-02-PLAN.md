---
phase: 04-match-discovery
plan: 02
type: execute
wave: 1
depends_on: []
files_modified:
  - src/scraper/discovery.py
  - tests/test_results_parser.py
  - pyproject.toml
autonomous: true

must_haves:
  truths:
    - "Parser extracts exactly 100 match entries from a standard results page"
    - "Parser correctly skips big-results entries on page 1 by selecting only elements with data-zonedgrouping-entry-unix"
    - "Parser extracts match_id, url, is_forfeit, and timestamp_ms for each entry"
    - "Parser detects forfeits via map-text == 'def'"
    - "Parser returns empty list for non-results HTML (graceful failure)"
  artifacts:
    - path: "src/scraper/discovery.py"
      provides: "DiscoveredMatch dataclass and parse_results_page function"
      exports: ["DiscoveredMatch", "parse_results_page"]
      min_lines: 40
    - path: "tests/test_results_parser.py"
      provides: "Unit tests for results page parser using real HTML samples"
      min_lines: 80
    - path: "pyproject.toml"
      provides: "beautifulsoup4 and lxml in project dependencies"
      contains: "beautifulsoup4"
  key_links:
    - from: "src/scraper/discovery.py"
      to: "beautifulsoup4"
      via: "from bs4 import BeautifulSoup"
      pattern: "from bs4 import BeautifulSoup"
    - from: "tests/test_results_parser.py"
      to: "data/recon/results-offset-0.html.gz"
      via: "loads real gzipped HTML samples for testing"
      pattern: "results-offset"
---

<objective>
Build a pure-function HTML parser that extracts match entries from HLTV results listing pages.

Purpose: The parser is the core extraction logic for Phase 4 discovery. It takes raw HTML and returns structured DiscoveredMatch objects. Using real HTML samples from Phase 3 recon ensures the selectors work against actual HLTV pages.

Output: DiscoveredMatch dataclass, parse_results_page() function, and comprehensive tests against real HTML samples.
</objective>

<execution_context>
@~/.claude/get-shit-done/workflows/execute-plan.md
@~/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/04-match-discovery/04-CONTEXT.md
@.planning/phases/04-match-discovery/04-RESEARCH.md
@.planning/phases/03-page-reconnaissance/recon/results-listing.md

Key source files:
@src/scraper/config.py (for HLTV_BASE_URL)
@pyproject.toml (to add dependencies)

HTML samples for testing (gzipped):
- data/recon/results-offset-0.html.gz (page 1, has big-results section)
- data/recon/results-offset-100.html.gz (page 2, no big-results)
- data/recon/results-offset-5000.html.gz (deep pagination)
</context>

<tasks>

<task type="auto">
  <name>Task 1: Add beautifulsoup4 and lxml to project dependencies</name>
  <files>pyproject.toml</files>
  <action>
    Add `beautifulsoup4>=4.12` and `lxml>=5.0` to the `[project] dependencies` list in pyproject.toml.

    The dependencies list should become:
    ```toml
    dependencies = [
        "nodriver>=0.38",
        "tenacity>=9.0",
        "beautifulsoup4>=4.12",
        "lxml>=5.0",
    ]
    ```

    Then run `pip install -e .` to install the updated dependencies. Both are likely already installed system-wide, but this makes them explicit project dependencies since HTML parsing is core functionality.
  </action>
  <verify>
    Run `python -c "from bs4 import BeautifulSoup; import lxml; print('OK')"` -- prints OK.
    Run `pip show beautifulsoup4 lxml` -- both show installed.
  </verify>
  <done>beautifulsoup4 and lxml are listed in pyproject.toml dependencies and importable.</done>
</task>

<task type="auto">
  <name>Task 2: ResultsPageParser implementation and unit tests with real HTML</name>
  <files>
    src/scraper/discovery.py
    tests/test_results_parser.py
  </files>
  <action>
    **1. Create `src/scraper/discovery.py`** with the parser:

    ```python
    """Match discovery from HLTV results listing pages.

    Provides:
    - DiscoveredMatch: dataclass for a single discovered match entry
    - parse_results_page: pure function extracting matches from results HTML
    """

    import logging
    import re
    from dataclasses import dataclass

    from bs4 import BeautifulSoup

    logger = logging.getLogger(__name__)

    @dataclass
    class DiscoveredMatch:
        """A match entry extracted from an HLTV results listing page."""
        match_id: int
        url: str            # Relative URL: /matches/2389953/furia-vs-b8-...
        is_forfeit: bool    # True when map-text == "def"
        timestamp_ms: int   # Unix milliseconds from data-zonedgrouping-entry-unix


    def parse_results_page(html: str) -> list[DiscoveredMatch]:
        """Parse an HLTV results listing page and return discovered matches.

        Uses the data-zonedgrouping-entry-unix attribute selector to select
        only regular entries. This automatically skips the big-results section
        on page 1 (those entries lack this attribute).

        Args:
            html: Raw HTML string of a results listing page.

        Returns:
            List of DiscoveredMatch objects. Typically 100 per page.
            Returns empty list if no entries found (e.g., non-results HTML).
        """
        soup = BeautifulSoup(html, "lxml")
        entries = soup.select(".result-con[data-zonedgrouping-entry-unix]")

        results = []
        for entry in entries:
            # Match URL and ID
            link = entry.select_one("a.a-reset")
            if link is None or not link.get("href"):
                logger.warning("Skipping entry: no a.a-reset link found")
                continue

            href = link["href"]
            m = re.search(r"/matches/(\d+)/", href)
            if not m:
                logger.warning("Skipping entry: no match ID in href %r", href)
                continue

            match_id = int(m.group(1))

            # Forfeit flag
            map_text_el = entry.select_one(".map-text")
            map_text = map_text_el.text.strip() if map_text_el else ""
            is_forfeit = (map_text == "def")

            # Timestamp
            timestamp_ms = int(entry["data-zonedgrouping-entry-unix"])

            results.append(DiscoveredMatch(
                match_id=match_id,
                url=href,
                is_forfeit=is_forfeit,
                timestamp_ms=timestamp_ms,
            ))

        return results
    ```

    **2. Create `tests/test_results_parser.py`** with tests using real HTML samples:

    The test file must load gzipped HTML from `data/recon/` using `gzip.decompress(Path(...).read_bytes()).decode("utf-8")`. Use a helper function or fixture for this.

    Important: The path to data/recon/ from the test file is `Path(__file__).resolve().parent.parent / "data" / "recon"`. Or use a fixture that constructs the path.

    **Test classes:**

    **TestParseResultsPageBasic:**
    - `test_parse_offset_0_returns_100_entries`: Load results-offset-0.html.gz, parse, assert len == 100. This is THE critical test -- page 1 has big-results (8 extra entries) that must be skipped.
    - `test_parse_offset_100_returns_100_entries`: Load results-offset-100.html.gz, parse, assert len == 100.
    - `test_parse_offset_5000_returns_100_entries`: Load results-offset-5000.html.gz, parse, assert len == 100.
    - `test_parse_empty_html_returns_empty`: Parse `"<html><body></body></html>"`, assert len == 0.
    - `test_parse_non_results_page_returns_empty`: Parse `"<html><body><div>Not a results page</div></body></html>"`, assert len == 0.

    **TestDiscoveredMatchFields:**
    - `test_match_id_is_positive_integer`: Parse offset-0, all match_ids > 0 and are ints
    - `test_url_starts_with_matches`: Parse offset-0, all urls start with "/matches/"
    - `test_url_contains_match_id`: Parse offset-0, for each entry: str(entry.match_id) appears in entry.url
    - `test_timestamp_ms_is_reasonable`: Parse offset-0, all timestamps are 13-digit integers (unix ms, post-2020: > 1577836800000)

    **TestForfeitDetection:**
    - `test_forfeit_detection`: Parse all three samples. Collect entries where is_forfeit is True. Verify that at least some non-forfeit entries exist (most should be False). If any forfeits happen to be in the samples, verify they are flagged. If none exist, just verify all entries have is_forfeit == False (valid -- not all pages have forfeits).

    **TestNoDuplicatesBigResults:**
    - `test_no_duplicate_match_ids_page_1`: Parse offset-0, collect all match_ids, verify len(set(match_ids)) == len(match_ids). This catches the big-results duplication bug.
    - `test_no_duplicate_match_ids_page_2`: Same for offset-100.

    Use `pytest.importorskip` or check file existence to gracefully skip if HTML samples are missing (they are gitignored). Pattern:
    ```python
    RECON_DIR = Path(__file__).resolve().parent.parent / "data" / "recon"

    def load_sample(filename: str) -> str:
        path = RECON_DIR / filename
        if not path.exists():
            pytest.skip(f"Sample HTML not found: {path}")
        return gzip.decompress(path.read_bytes()).decode("utf-8")
    ```
  </action>
  <verify>
    Run `python -m pytest tests/test_results_parser.py -v` -- all tests pass.
    Run `python -c "from scraper.discovery import DiscoveredMatch, parse_results_page; print('imports OK')"` -- succeeds.
    Verify exactly 100 entries returned from each sample page (most critical check).
  </verify>
  <done>
    - DiscoveredMatch dataclass has match_id, url, is_forfeit, timestamp_ms fields
    - parse_results_page() extracts exactly 100 entries from each sample page
    - Big-results on page 1 are correctly skipped (no duplicate match_ids)
    - Forfeits detected via map-text == "def"
    - Empty/non-results HTML returns empty list gracefully
    - All tests pass against real HTML samples from data/recon/
  </done>
</task>

</tasks>

<verification>
1. `python -m pytest tests/test_results_parser.py -v` -- all parser tests pass
2. `python -m pytest tests/ -m "not integration" -v` -- full unit test suite passes
3. `python -c "from scraper.discovery import parse_results_page; print('OK')"` -- import works
4. Verify offset-0 page returns exactly 100 entries (not 108)
5. Verify no duplicate match_ids in any parsed page
</verification>

<success_criteria>
- DiscoveredMatch dataclass defined with 4 fields (match_id, url, is_forfeit, timestamp_ms)
- parse_results_page() returns exactly 100 entries per page
- Page 1 big-results are skipped (no duplicates)
- Forfeits detected correctly
- beautifulsoup4 and lxml added to pyproject.toml
- All tests pass against real HTML samples
</success_criteria>

<output>
After completion, create `.planning/phases/04-match-discovery/04-02-SUMMARY.md`
</output>
