---
phase: 07-performance-and-economy-extraction
plan: 04
type: execute
wave: 2
depends_on: ["07-01", "07-02", "07-03"]
files_modified:
  - src/scraper/performance_economy.py
  - tests/test_performance_economy.py
autonomous: true

must_haves:
  truths:
    - "Orchestrator fetches both performance AND economy pages for each pending map in a single batch"
    - "Performance data updates existing player_stats rows (kpr, dpr, impact/mk_rating populated)"
    - "Economy data inserts into economy table with correct FK references to round_history"
    - "Kill matrix data inserts into kill_matrix table with correct player pair mappings"
    - "Fetch failure discards entire batch; parse failure is per-map and does not halt the batch"
    - "Only maps with existing player_stats (Phase 6 complete) and NULL kpr (Phase 7 not yet run) are processed"
  artifacts:
    - path: "src/scraper/performance_economy.py"
      provides: "Async orchestrator for performance and economy extraction"
      exports: ["run_performance_economy"]
      min_lines: 100
    - path: "tests/test_performance_economy.py"
      provides: "Orchestrator tests with mocked client and real DB"
      min_lines: 120
  key_links:
    - from: "src/scraper/performance_economy.py"
      to: "src/scraper/performance_parser.py"
      via: "Imports parse_performance for HTML parsing"
      pattern: "from scraper.performance_parser import parse_performance"
    - from: "src/scraper/performance_economy.py"
      to: "src/scraper/economy_parser.py"
      via: "Imports parse_economy for HTML parsing"
      pattern: "from scraper.economy_parser import parse_economy"
    - from: "src/scraper/performance_economy.py"
      to: "src/scraper/repository.py"
      via: "Calls get_pending_perf_economy and upsert_perf_economy_complete"
      pattern: "get_pending_perf_economy|upsert_perf_economy_complete"
    - from: "src/scraper/performance_economy.py"
      to: "src/scraper/storage.py"
      via: "Uses map_performance and map_economy page types for save/load"
      pattern: "map_performance|map_economy"
---

<objective>
Build a single async orchestrator that fetches both performance and economy pages per map, parses them, and persists all data atomically.

Purpose: This orchestrator wires together the performance parser, economy parser, repository, and storage layers into a batch pipeline. It follows the established Phase 6 pattern: fetch-first batch strategy with per-map parse/persist failure handling. One orchestrator for both page types avoids duplicate batch management.
Output: `src/scraper/performance_economy.py` (async orchestrator) + comprehensive test suite with mocked client and real DB.
</objective>

<execution_context>
@~/.claude/get-shit-done/workflows/execute-plan.md
@~/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/07-performance-and-economy-extraction/07-RESEARCH.md

# Prior plan summaries (needed for this plan)
@.planning/phases/07-performance-and-economy-extraction/07-01-SUMMARY.md
@.planning/phases/07-performance-and-economy-extraction/07-02-SUMMARY.md
@.planning/phases/07-performance-and-economy-extraction/07-03-SUMMARY.md

# Pattern reference (follow this closely)
@src/scraper/map_stats.py
@tests/test_map_stats.py

# Dependencies
@src/scraper/performance_parser.py
@src/scraper/economy_parser.py
@src/scraper/repository.py
@src/scraper/storage.py
@src/scraper/config.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Performance + economy orchestrator</name>
  <files>src/scraper/performance_economy.py</files>
  <action>
Create `src/scraper/performance_economy.py` following the `map_stats.py` orchestrator pattern closely.

**Module-level constants:**
```python
PARSER_VERSION = "perf_economy_v1"
PERF_URL_TEMPLATE = "/stats/matches/performance/mapstatsid/{mapstatsid}/x"
ECON_URL_TEMPLATE = "/stats/matches/economy/mapstatsid/{mapstatsid}/x"
```

**Main function:**
```python
async def run_performance_economy(
    client,         # HLTVClient
    match_repo,     # MatchRepository
    storage,        # HtmlStorage
    config,         # ScraperConfig
) -> dict:
```

**Implementation (follows map_stats.py pattern exactly):**

1. **Get pending maps:** `match_repo.get_pending_perf_economy(limit=config.perf_economy_batch_size)`. Returns list of `{match_id, map_number, mapstatsid}` dicts.

2. **Fetch phase -- fetch ALL pages (both perf + econ for each map), discard batch on any failure:**
   For each pending entry:
   - Build performance URL: `config.base_url + PERF_URL_TEMPLATE.format(mapstatsid=entry["mapstatsid"])`
   - Build economy URL: `config.base_url + ECON_URL_TEMPLATE.format(mapstatsid=entry["mapstatsid"])`
   - Fetch performance page via `client.fetch(perf_url)`
   - Save via `storage.save(html, match_id=..., page_type="map_performance", mapstatsid=...)`
   - Fetch economy page via `client.fetch(econ_url)`
   - Save via `storage.save(html, match_id=..., page_type="map_economy", mapstatsid=...)`
   - On ANY fetch exception: log error, set `stats["fetch_errors"] += 1`, return stats immediately (discard entire batch)
   - Append entry to `fetched_entries` on success

3. **Parse + persist phase -- per-map failure handling:**
   For each fetched entry:
   - Load performance HTML via `storage.load(match_id=..., page_type="map_performance", mapstatsid=...)`
   - Load economy HTML via `storage.load(match_id=..., page_type="map_economy", mapstatsid=...)`
   - Parse performance: `perf_result = parse_performance(perf_html, mapstatsid)`
   - Parse economy: `econ_result = parse_economy(econ_html, mapstatsid)`
   - Build `perf_stats` dicts for UPSERT_PLAYER_STATS -- this UPDATES existing rows:
     For each player in `perf_result.players`:
     ```python
     {
         "match_id": match_id,
         "map_number": map_number,
         "player_id": p.player_id,
         "player_name": p.player_name,
         "team_id": None,  # Not available on perf page; existing value preserved by UPSERT
         # Existing Phase 6 values -- set to None so UPSERT doesn't overwrite with NULL.
         # IMPORTANT: The UPSERT ON CONFLICT will only update columns where excluded.col IS NOT NULL
         # Actually, UPSERT unconditionally sets all listed columns. So we need to handle this.
         # APPROACH: Use a separate UPDATE SQL that only sets Phase 7 columns, not a full UPSERT.
         # OR: Read existing row values first.
         # SIMPLEST: Use a targeted UPDATE statement that only touches kpr, dpr, impact, mk_rating.
     }
     ```

   CRITICAL DESIGN DECISION: The full UPSERT_PLAYER_STATS would overwrite Phase 6 values (kills, deaths, etc.) with NULL because the performance parser doesn't have those fields. Instead, create a targeted UPDATE approach:

   Build the update as a separate SQL statement or use the existing UPSERT but populate ALL fields. Since we need ALL columns for the UPSERT, we should:
   a) First read existing player_stats for this map to get Phase 6 values
   b) Merge Phase 7 values onto the existing dict
   c) Pass the merged dict to UPSERT_PLAYER_STATS

   Implementation:
   ```python
   existing_stats = match_repo.get_player_stats(match_id, map_number)
   existing_by_pid = {s["player_id"]: s for s in existing_stats}

   perf_stats = []
   for p in perf_result.players:
       base = existing_by_pid.get(p.player_id, {})
       perf_stats.append({
           "match_id": match_id,
           "map_number": map_number,
           "player_id": p.player_id,
           "player_name": base.get("player_name", p.player_name),
           "team_id": base.get("team_id"),
           "kills": base.get("kills"),
           "deaths": base.get("deaths"),
           "assists": base.get("assists"),
           "flash_assists": base.get("flash_assists"),
           "hs_kills": base.get("hs_kills"),
           "kd_diff": base.get("kd_diff"),
           "adr": base.get("adr"),
           "kast": base.get("kast"),
           "fk_diff": base.get("fk_diff"),
           "rating_2": base.get("rating_2"),
           "rating_3": base.get("rating_3"),
           "kpr": p.kpr,
           "dpr": p.dpr,
           "impact": p.impact,
           "mk_rating": p.mk_rating,
           "opening_kills": base.get("opening_kills"),
           "opening_deaths": base.get("opening_deaths"),
           "multi_kills": base.get("multi_kills"),
           "clutch_wins": base.get("clutch_wins"),
           "traded_deaths": base.get("traded_deaths"),
           "round_swing": base.get("round_swing"),
           "scraped_at": now,
           "source_url": perf_source_url,
           "parser_version": PARSER_VERSION,
       })
   ```

   - Build `economy_data` dicts for UPSERT_ECONOMY:
     For each round_entry in `econ_result.rounds`:
     Need `team_id` -- the economy table uses team_id (integer), but the parser returns team_name (string). Resolution: look up team IDs from the existing player_stats or match data.
     ```python
     # Get team IDs from match record
     match_data = match_repo.get_match(match_id)
     team_name_to_id = {
         match_data["team1_name"]: match_data["team1_id"],
         match_data["team2_name"]: match_data["team2_id"],
     }
     ```
     Then for each round entry:
     ```python
     {
         "match_id": match_id,
         "map_number": map_number,
         "round_number": r.round_number,
         "team_id": team_name_to_id.get(r.team_name, 0),
         "equipment_value": r.equipment_value,
         "buy_type": r.buy_type,
         "scraped_at": now,
         "source_url": econ_source_url,
         "parser_version": PARSER_VERSION,
     }
     ```
     IMPORTANT: Only insert economy rows for rounds that exist in round_history (FK constraint). Filter: `round_number <= round_count_from_round_history`. Get this from `match_repo` or simply rely on the FK to reject and catch the error. Safer: query round_history count first.

   - Build `kill_matrix_data` dicts for UPSERT_KILL_MATRIX:
     ```python
     {
         "match_id": match_id,
         "map_number": map_number,
         "matrix_type": km.matrix_type,
         "player1_id": km.player1_id,
         "player2_id": km.player2_id,
         "player1_kills": km.player1_kills,
         "player2_kills": km.player2_kills,
         "scraped_at": now,
         "source_url": perf_source_url,
         "parser_version": PARSER_VERSION,
     }
     ```

   - Persist atomically: `match_repo.upsert_perf_economy_complete(perf_stats, economy_data, kill_matrix_data)`

   - On exception: log error, increment `stats["failed"]`, continue to next map.

4. **Return stats dict:** `{"batch_size": N, "fetched": N, "parsed": N, "failed": N, "fetch_errors": N}`

**Logging:** Use `logger = logging.getLogger(__name__)`. Log batch start, per-map success, per-map failure, batch summary.
  </action>
  <verify>
`python -c "from scraper.performance_economy import run_performance_economy, PARSER_VERSION; print('Import OK, version:', PARSER_VERSION)"` succeeds.
  </verify>
  <done>Orchestrator module exists with run_performance_economy() async function that fetches both page types, parses with both parsers, and persists via repository. Follows map_stats.py pattern with fetch-first batch strategy.</done>
</task>

<task type="auto">
  <name>Task 2: Orchestrator test suite</name>
  <files>tests/test_performance_economy.py</files>
  <action>
Create `tests/test_performance_economy.py` following the `tests/test_map_stats.py` pattern closely.

**Test infrastructure:**
- Use `tmp_db` fixture (Database with all migrations applied, including 004)
- Use `match_repo` fixture (MatchRepository on tmp_db)
- Use `storage` fixture (HtmlStorage in tmp_path)
- Use `mock_client` fixture (AsyncMock for HLTVClient)
- Use real performance + economy HTML samples from data/recon/ (sample 164779)

**Seed helper function:**
```python
def seed_match_with_map_stats(match_repo, match_id, mapstatsid, map_number=1):
    """Seed a match + map + player_stats + round_history so Phase 7 can run."""
```
This must:
- Insert a match record (with team1_id, team1_name, team2_id, team2_name)
- Insert a map record (match_id, map_number, mapstatsid)
- Insert 10 player_stats rows (using parse_map_stats on the map stats sample to get real data)
- Insert round_history rows (also from parse_map_stats)
This simulates Phase 6 having completed for this map.

Use the map stats sample `mapstats-164779-stats.html.gz` for seeding. The performance sample `performance-164779.html.gz` and economy sample `economy-164779.html.gz` are for the same mapstatsid.

**Mock client setup:**
The mock client's `fetch` method should return the correct HTML based on the URL:
- URLs containing "/performance/" return the performance HTML
- URLs containing "/economy/" return the economy HTML
Use `side_effect` to inspect the URL and return accordingly.

**Test classes:**

1. `TestHappyPath`:
   - `test_processes_pending_map` -- seed map, run orchestrator, verify stats show parsed=1
   - `test_kpr_populated` -- after run, `get_player_stats()` returns rows with non-None kpr
   - `test_dpr_populated` -- rows have non-None dpr
   - `test_impact_or_mk_rating_populated` -- rows have either impact or mk_rating non-None (depends on rating version)
   - `test_economy_rows_created` -- economy table has rows for this match/map
   - `test_economy_has_both_teams` -- economy rows contain entries for 2 different team_ids
   - `test_kill_matrix_created` -- kill_matrix table has rows for this match/map
   - `test_kill_matrix_three_types` -- kill_matrix has entries for "all", "first_kill", "awp"
   - `test_existing_stats_preserved` -- Phase 6 values (kills, deaths, etc.) are NOT overwritten to NULL

2. `TestFetchFailure`:
   - `test_fetch_error_discards_batch` -- mock client raises on first fetch, stats show fetch_errors=1, parsed=0
   - `test_no_data_persisted_on_fetch_error` -- kpr remains NULL, no economy rows

3. `TestParseFailure`:
   - `test_parse_error_continues` -- seed 2 maps, mock returns invalid HTML for first and valid for second. Stats show failed=1, parsed=1.

4. `TestNoPendingMaps`:
   - `test_empty_batch` -- no seeded data, stats show batch_size=0

5. `TestAlreadyProcessed`:
   - `test_skips_already_processed` -- seed map, manually update kpr to non-None. Verify get_pending_perf_economy returns empty. Orchestrator returns batch_size=0.

All async tests use `@pytest.mark.asyncio`.
  </action>
  <verify>
`pytest tests/test_performance_economy.py -v --tb=short` -- all tests pass.
  </verify>
  <done>Orchestrator test suite covers happy path (all data persisted correctly), fetch failure (batch discarded), parse failure (per-map), empty batch, and already-processed skipping. All tests pass.</done>
</task>

</tasks>

<verification>
1. `pytest tests/test_performance_economy.py -v --tb=short` -- all tests pass
2. `pytest tests/ -v --tb=short -x` -- full test suite passes (no regressions from any phase)
3. Manual verification: `python -c "from scraper.performance_economy import run_performance_economy; print('Full module chain importable')"` confirms all dependencies resolve
</verification>

<success_criteria>
- run_performance_economy() fetches both performance and economy pages per map
- Performance data correctly merges onto existing player_stats rows (Phase 6 values preserved)
- Economy data inserted with correct team_id references (resolved from match record)
- Kill matrix data inserted with correct player pair mappings
- Fetch failure discards entire batch (entries remain pending)
- Parse failure is per-map (other maps continue)
- Maps already processed (kpr not NULL) are not re-fetched
- All existing tests from prior phases continue to pass
</success_criteria>

<output>
After completion, create `.planning/phases/07-performance-and-economy-extraction/07-04-SUMMARY.md`
</output>
