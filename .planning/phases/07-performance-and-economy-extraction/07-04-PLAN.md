---
phase: 07-performance-and-economy-extraction
plan: 04
type: execute
wave: 2
depends_on: ["07-01", "07-02", "07-03"]
files_modified:
  - src/scraper/performance_economy.py
  - tests/test_performance_economy.py
autonomous: true

must_haves:
  truths:
    - "Orchestrator fetches both performance AND economy pages for each pending map in a single batch"
    - "Performance data updates existing player_stats rows (kpr, dpr, impact/mk_rating populated) without overwriting Phase 6 values"
    - "Economy data inserts into economy table only for round numbers that exist in round_history (FK-safe)"
    - "Kill matrix data inserts into kill_matrix table with correct player pair mappings"
    - "Fetch failure discards entire batch; parse failure is per-map and does not halt the batch"
    - "Only maps with existing player_stats (Phase 6 complete) and NULL kpr (Phase 7 not yet run) are processed"
  artifacts:
    - path: "src/scraper/performance_economy.py"
      provides: "Async orchestrator for performance and economy extraction"
      exports: ["run_performance_economy"]
      min_lines: 100
    - path: "tests/test_performance_economy.py"
      provides: "Orchestrator tests with mocked client and real DB"
      min_lines: 120
  key_links:
    - from: "src/scraper/performance_economy.py"
      to: "src/scraper/performance_parser.py"
      via: "Imports parse_performance for HTML parsing"
      pattern: "from scraper.performance_parser import parse_performance"
    - from: "src/scraper/performance_economy.py"
      to: "src/scraper/economy_parser.py"
      via: "Imports parse_economy for HTML parsing"
      pattern: "from scraper.economy_parser import parse_economy"
    - from: "src/scraper/performance_economy.py"
      to: "src/scraper/repository.py"
      via: "Calls get_pending_perf_economy, get_player_stats, get_valid_round_numbers, and upsert_perf_economy_complete"
      pattern: "get_pending_perf_economy|get_player_stats|get_valid_round_numbers|upsert_perf_economy_complete"
    - from: "src/scraper/performance_economy.py"
      to: "src/scraper/storage.py"
      via: "Uses map_performance and map_economy page types for save/load"
      pattern: "map_performance|map_economy"
---

<objective>
Build a single async orchestrator that fetches both performance and economy pages per map, parses them, and persists all data atomically.

Purpose: This orchestrator wires together the performance parser, economy parser, repository, and storage layers into a batch pipeline. It follows the established Phase 6 pattern: fetch-first batch strategy with per-map parse/persist failure handling. One orchestrator for both page types avoids duplicate batch management.
Output: `src/scraper/performance_economy.py` (async orchestrator) + comprehensive test suite with mocked client and real DB.
</objective>

<execution_context>
@~/.claude/get-shit-done/workflows/execute-plan.md
@~/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/07-performance-and-economy-extraction/07-RESEARCH.md

# Prior plan summaries (needed for this plan)
@.planning/phases/07-performance-and-economy-extraction/07-01-SUMMARY.md
@.planning/phases/07-performance-and-economy-extraction/07-02-SUMMARY.md
@.planning/phases/07-performance-and-economy-extraction/07-03-SUMMARY.md

# Pattern reference (follow this closely)
@src/scraper/map_stats.py
@tests/test_map_stats.py

# Dependencies
@src/scraper/performance_parser.py
@src/scraper/economy_parser.py
@src/scraper/repository.py
@src/scraper/storage.py
@src/scraper/config.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Performance + economy orchestrator</name>
  <files>src/scraper/performance_economy.py</files>
  <action>
Create `src/scraper/performance_economy.py` following the `map_stats.py` orchestrator pattern closely.

**Module-level constants:**
```python
PARSER_VERSION = "perf_economy_v1"
PERF_URL_TEMPLATE = "/stats/matches/performance/mapstatsid/{mapstatsid}/x"
ECON_URL_TEMPLATE = "/stats/matches/economy/mapstatsid/{mapstatsid}/x"
```

**Main function:**
```python
async def run_performance_economy(
    client,         # HLTVClient
    match_repo,     # MatchRepository
    storage,        # HtmlStorage
    config,         # ScraperConfig
) -> dict:
```

**Implementation (follows map_stats.py pattern exactly):**

1. **Get pending maps:** `match_repo.get_pending_perf_economy(limit=config.perf_economy_batch_size)`. Returns list of `{match_id, map_number, mapstatsid}` dicts.

2. **Fetch phase -- fetch ALL pages (both perf + econ for each map), discard batch on any failure:**
   For each pending entry:
   - Build performance URL: `config.base_url + PERF_URL_TEMPLATE.format(mapstatsid=entry["mapstatsid"])`
   - Build economy URL: `config.base_url + ECON_URL_TEMPLATE.format(mapstatsid=entry["mapstatsid"])`
   - Fetch performance page via `client.fetch(perf_url)`
   - Save via `storage.save(html, match_id=..., page_type="map_performance", mapstatsid=...)`
   - Fetch economy page via `client.fetch(econ_url)`
   - Save via `storage.save(html, match_id=..., page_type="map_economy", mapstatsid=...)`
   - On ANY fetch exception: log error, set `stats["fetch_errors"] += 1`, return stats immediately (discard entire batch)
   - Append entry to `fetched_entries` on success

3. **Parse + persist phase -- per-map failure handling:**
   For each fetched entry:
   - Load performance HTML via `storage.load(match_id=..., page_type="map_performance", mapstatsid=...)`
   - Load economy HTML via `storage.load(match_id=..., page_type="map_economy", mapstatsid=...)`
   - Parse performance: `perf_result = parse_performance(perf_html, mapstatsid)`
   - Parse economy: `econ_result = parse_economy(econ_html, mapstatsid)`

   **Build perf_stats dicts (read-merge approach to preserve Phase 6 data):**

   The UPSERT_PLAYER_STATS sets ALL listed columns from the excluded row. If we pass NULL for Phase 6 columns (kills, deaths, etc.), the UPSERT would overwrite them with NULL. Therefore we MUST read existing values and merge.

   ```python
   existing_stats = match_repo.get_player_stats(match_id, map_number)
   existing_by_pid = {s["player_id"]: s for s in existing_stats}

   perf_stats = []
   for p in perf_result.players:
       base = existing_by_pid.get(p.player_id, {})
       # Every column in the UPSERT must have a non-NULL value passed
       # (either from existing Phase 6 data or from Phase 7 parser).
       # If a player from the performance page doesn't exist in Phase 6
       # data (shouldn't happen, but be defensive), use None for Phase 6
       # columns -- the UPSERT will INSERT a new row with those NULLs.
       perf_stats.append({
           "match_id": match_id,
           "map_number": map_number,
           "player_id": p.player_id,
           "player_name": base.get("player_name", p.player_name),
           "team_id": base.get("team_id"),
           "kills": base.get("kills"),
           "deaths": base.get("deaths"),
           "assists": base.get("assists"),
           "flash_assists": base.get("flash_assists"),
           "hs_kills": base.get("hs_kills"),
           "kd_diff": base.get("kd_diff"),
           "adr": base.get("adr"),
           "kast": base.get("kast"),
           "fk_diff": base.get("fk_diff"),
           "rating_2": base.get("rating_2"),
           "rating_3": base.get("rating_3"),
           # Phase 7 fields from performance parser:
           "kpr": p.kpr,
           "dpr": p.dpr,
           "impact": p.impact,
           "mk_rating": p.mk_rating,
           # Phase 6 fields preserved from existing row:
           "opening_kills": base.get("opening_kills"),
           "opening_deaths": base.get("opening_deaths"),
           "multi_kills": base.get("multi_kills"),
           "clutch_wins": base.get("clutch_wins"),
           "traded_deaths": base.get("traded_deaths"),
           "round_swing": base.get("round_swing"),
           "scraped_at": now,
           "source_url": perf_source_url,
           "parser_version": PARSER_VERSION,
       })
   ```

   Note: `get_player_stats(match_id, map_number)` already exists in repository.py (added in Phase 6). It returns `list[dict]` via `sqlite3.Row`, which includes ALL columns. The `base.get()` calls safely return None for any column that doesn't exist yet (defensive).

   **Resolve team_id for economy data (from player_stats, not match names):**

   The economy parser returns `team_name` (from FusionChart `seriesname`) but the economy table needs `team_id` (integer). FusionChart team names may not exactly match the team names stored in the matches table (abbreviations, unicode differences, etc.).

   Instead of fragile name matching against the matches table, resolve team_ids from the player_stats rows that already exist (Phase 6 persisted them with correct team_ids):

   ```python
   # Get distinct team_ids and their names from player_stats for this map
   team_ids_from_stats = {}
   for s in existing_stats:
       if s["team_id"] and s["team_id"] not in team_ids_from_stats:
           team_ids_from_stats[s["team_id"]] = s.get("player_name", "")

   # Build name->id mapping using match data as fallback for names
   match_data = match_repo.get_match(match_id)
   team_name_to_id = {}
   if match_data:
       team_name_to_id[match_data["team1_name"]] = match_data["team1_id"]
       team_name_to_id[match_data["team2_name"]] = match_data["team2_id"]

   # Try FusionChart seriesname against match team names
   # If exact match fails, use positional mapping (first dataset = team1, second = team2)
   def resolve_team_id(team_name: str, dataset_index: int) -> int | None:
       # Try exact name match first
       if team_name in team_name_to_id:
           return team_name_to_id[team_name]
       # Fallback: positional (dataset[0] = team1, dataset[1] = team2)
       if match_data:
           if dataset_index == 0:
               return match_data["team1_id"]
           elif dataset_index == 1:
               return match_data["team2_id"]
       return None
   ```

   Actually, the simpler and more reliable approach: the economy parser's EconomyData has `team1_name` and `team2_name` (from dataset[0] and dataset[1] seriesname). Map these positionally to team_ids from player_stats:

   ```python
   # Get the two distinct team_ids from existing player_stats
   distinct_team_ids = list({s["team_id"] for s in existing_stats if s["team_id"] is not None})
   # There should be exactly 2 team_ids
   if len(distinct_team_ids) != 2:
       raise ValueError(f"Expected 2 team_ids in player_stats, got {len(distinct_team_ids)}")

   # Map economy team names to team_ids
   # Use match_data for the name->id mapping since player_stats doesn't store team_name
   team_name_to_id = {}
   if match_data:
       team_name_to_id[match_data["team1_name"]] = match_data["team1_id"]
       team_name_to_id[match_data["team2_name"]] = match_data["team2_id"]

   # For economy, resolve by name first, fall back to positional order
   econ_team1_id = team_name_to_id.get(econ_result.team1_name)
   econ_team2_id = team_name_to_id.get(econ_result.team2_name)

   # If name matching fails for either team, use positional mapping
   # The economy FusionChart dataset order matches team1/team2 from the match
   if econ_team1_id is None or econ_team2_id is None:
       logger.warning(
           "Economy team name mismatch for match %d: economy has '%s'/'%s', "
           "match has '%s'/'%s'. Using positional fallback.",
           match_id, econ_result.team1_name, econ_result.team2_name,
           match_data.get("team1_name", "?") if match_data else "?",
           match_data.get("team2_name", "?") if match_data else "?",
       )
       if match_data:
           econ_team1_id = match_data["team1_id"]
           econ_team2_id = match_data["team2_id"]

   econ_name_to_id = {
       econ_result.team1_name: econ_team1_id,
       econ_result.team2_name: econ_team2_id,
   }
   ```

   **Filter economy rows to valid round numbers (FK constraint handling):**

   The economy table has `FOREIGN KEY (match_id, map_number, round_number) REFERENCES round_history(...)`. Economy data from the FusionChart may include rounds that don't exist in round_history (e.g., if round_history has fewer rounds due to parsing differences, or MR12 OT edge cases).

   Query the valid round numbers BEFORE building economy dicts, and filter:

   ```python
   valid_rounds = match_repo.get_valid_round_numbers(match_id, map_number)

   economy_data = []
   skipped_rounds = 0
   for r in econ_result.rounds:
       if r.round_number not in valid_rounds:
           skipped_rounds += 1
           continue
       team_id = econ_name_to_id.get(r.team_name)
       if team_id is None:
           logger.warning("Cannot resolve team_id for '%s', skipping", r.team_name)
           continue
       economy_data.append({
           "match_id": match_id,
           "map_number": map_number,
           "round_number": r.round_number,
           "team_id": team_id,
           "equipment_value": r.equipment_value,
           "buy_type": r.buy_type,
           "scraped_at": now,
           "source_url": econ_source_url,
           "parser_version": PARSER_VERSION,
       })
   if skipped_rounds > 0:
       logger.info(
           "Skipped %d economy rounds not in round_history for match %d map %d",
           skipped_rounds, match_id, map_number,
       )
   ```

   **Build kill_matrix_data dicts:**
   ```python
   kill_matrix_data = []
   for km in perf_result.kill_matrix:
       kill_matrix_data.append({
           "match_id": match_id,
           "map_number": map_number,
           "matrix_type": km.matrix_type,
           "player1_id": km.player1_id,
           "player2_id": km.player2_id,
           "player1_kills": km.player1_kills,
           "player2_kills": km.player2_kills,
           "scraped_at": now,
           "source_url": perf_source_url,
           "parser_version": PARSER_VERSION,
       })
   ```

   **Persist atomically:** `match_repo.upsert_perf_economy_complete(perf_stats, economy_data, kill_matrix_data)`

   On exception: log error, increment `stats["failed"]`, continue to next map.

4. **Return stats dict:** `{"batch_size": N, "fetched": N, "parsed": N, "failed": N, "fetch_errors": N}`

**Logging:** Use `logger = logging.getLogger(__name__)`. Log batch start, per-map success, per-map failure, batch summary.
  </action>
  <verify>
`python -c "from scraper.performance_economy import run_performance_economy, PARSER_VERSION; print('Import OK, version:', PARSER_VERSION)"` succeeds.
  </verify>
  <done>Orchestrator module exists with run_performance_economy() async function that fetches both page types, parses with both parsers, merges performance data onto existing player_stats (preserving Phase 6 values), filters economy rows to valid round_history entries, resolves team_ids from player_stats, and persists via repository. Follows map_stats.py pattern with fetch-first batch strategy.</done>
</task>

<task type="auto">
  <name>Task 2: Orchestrator test suite</name>
  <files>tests/test_performance_economy.py</files>
  <action>
Create `tests/test_performance_economy.py` following the `tests/test_map_stats.py` pattern closely.

**Test infrastructure:**
- Use `tmp_db` fixture (Database with all migrations applied, including 004)
- Use `match_repo` fixture (MatchRepository on tmp_db)
- Use `storage` fixture (HtmlStorage in tmp_path)
- Use `mock_client` fixture (AsyncMock for HLTVClient)
- Use real performance + economy HTML samples from data/recon/ (sample 164779)

**Seed helper function:**
```python
def seed_match_with_map_stats(match_repo, match_id, mapstatsid, map_number=1):
    """Seed a match + map + player_stats + round_history so Phase 7 can run."""
```
This must:
- Insert a match record (with team1_id, team1_name, team2_id, team2_name)
- Insert a map record (match_id, map_number, mapstatsid)
- Insert 10 player_stats rows (using parse_map_stats on the map stats sample to get real data)
- Insert round_history rows (also from parse_map_stats)
This simulates Phase 6 having completed for this map.

Use the map stats sample `mapstats-164779-stats.html.gz` for seeding. The performance sample `performance-164779.html.gz` and economy sample `economy-164779.html.gz` are for the same mapstatsid.

**Mock client setup:**
The mock client's `fetch` method should return the correct HTML based on the URL:
- URLs containing "/performance/" return the performance HTML
- URLs containing "/economy/" return the economy HTML
Use `side_effect` to inspect the URL and return accordingly.

**Test classes:**

1. `TestHappyPath`:
   - `test_processes_pending_map` -- seed map, run orchestrator, verify stats show parsed=1
   - `test_kpr_populated` -- after run, `get_player_stats()` returns rows with non-None kpr
   - `test_dpr_populated` -- rows have non-None dpr
   - `test_impact_or_mk_rating_populated` -- rows have either impact or mk_rating non-None (depends on rating version)
   - `test_economy_rows_created` -- economy table has rows for this match/map
   - `test_economy_has_both_teams` -- economy rows contain entries for 2 different team_ids
   - `test_kill_matrix_created` -- kill_matrix table has rows for this match/map
   - `test_kill_matrix_three_types` -- kill_matrix has entries for "all", "first_kill", "awp"
   - `test_existing_stats_preserved` -- CRITICAL: After Phase 7 runs, verify that Phase 6 values (kills, deaths, assists, flash_assists, hs_kills, kd_diff, adr, kast, fk_diff, rating_2 or rating_3, opening_kills, opening_deaths, multi_kills, clutch_wins, traded_deaths) are NOT NULL. Compare specific values before and after to confirm they are unchanged. Implementation:
     ```python
     # Before Phase 7
     before = match_repo.get_player_stats(match_id, map_number)
     before_by_pid = {s["player_id"]: s for s in before}
     # Run orchestrator
     await run_performance_economy(mock_client, match_repo, storage, config)
     # After Phase 7
     after = match_repo.get_player_stats(match_id, map_number)
     for row in after:
         pid = row["player_id"]
         prev = before_by_pid[pid]
         # Phase 6 fields must be preserved exactly
         assert row["kills"] == prev["kills"], f"kills overwritten for player {pid}"
         assert row["deaths"] == prev["deaths"], f"deaths overwritten for player {pid}"
         assert row["assists"] == prev["assists"]
         assert row["team_id"] == prev["team_id"]
         assert row["opening_kills"] == prev["opening_kills"]
         assert row["opening_deaths"] == prev["opening_deaths"]
         assert row["multi_kills"] == prev["multi_kills"]
         assert row["clutch_wins"] == prev["clutch_wins"]
         assert row["traded_deaths"] == prev["traded_deaths"]
         # Phase 7 fields must NOW be populated
         assert row["kpr"] is not None, f"kpr still NULL for player {pid}"
     ```

2. `TestEconomyFKFiltering`:
   - `test_economy_rounds_match_round_history` -- after run, every economy row's round_number exists in round_history for that map. Query both tables and assert round_number is a subset.
   - `test_economy_skips_invalid_rounds` -- seed a map where round_history has fewer rounds than the economy FusionChart data would contain (e.g., delete some round_history rows after seeding). Verify that economy rows are only created for the remaining valid round numbers, no FK errors raised.

3. `TestFetchFailure`:
   - `test_fetch_error_discards_batch` -- mock client raises on first fetch, stats show fetch_errors=1, parsed=0
   - `test_no_data_persisted_on_fetch_error` -- kpr remains NULL, no economy rows

4. `TestParseFailure`:
   - `test_parse_error_continues` -- seed 2 maps, mock returns invalid HTML for first and valid for second. Stats show failed=1, parsed=1.

5. `TestNoPendingMaps`:
   - `test_empty_batch` -- no seeded data, stats show batch_size=0

6. `TestAlreadyProcessed`:
   - `test_skips_already_processed` -- seed map, manually update kpr to non-None. Verify get_pending_perf_economy returns empty. Orchestrator returns batch_size=0.

All async tests use `@pytest.mark.asyncio`.
  </action>
  <verify>
`pytest tests/test_performance_economy.py -v --tb=short` -- all tests pass.
  </verify>
  <done>Orchestrator test suite covers happy path (all data persisted correctly, Phase 6 values preserved), economy FK filtering (only valid round numbers inserted), fetch failure (batch discarded), parse failure (per-map), empty batch, and already-processed skipping. All tests pass.</done>
</task>

</tasks>

<verification>
1. `pytest tests/test_performance_economy.py -v --tb=short` -- all tests pass
2. `pytest tests/ -v --tb=short -x` -- full test suite passes (no regressions from any phase)
3. Manual verification: `python -c "from scraper.performance_economy import run_performance_economy; print('Full module chain importable')"` confirms all dependencies resolve
</verification>

<success_criteria>
- run_performance_economy() fetches both performance and economy pages per map
- Performance data correctly merges onto existing player_stats rows via read-merge-write (Phase 6 values kills/deaths/assists/etc. are preserved, not overwritten with NULL)
- Economy data inserted only for rounds that exist in round_history (FK-safe filtering via get_valid_round_numbers)
- Team IDs for economy resolved from player_stats team_ids with match_data name-match and positional fallback (not fragile FusionChart name matching alone)
- Kill matrix data inserted with correct player pair mappings
- Fetch failure discards entire batch (entries remain pending)
- Parse failure is per-map (other maps continue)
- Maps already processed (kpr not NULL) are not re-fetched
- test_existing_stats_preserved explicitly verifies Phase 6 columns are not overwritten
- test_economy_rounds_match_round_history verifies FK safety
- All existing tests from prior phases continue to pass
</success_criteria>

<output>
After completion, create `.planning/phases/07-performance-and-economy-extraction/07-04-SUMMARY.md`
</output>
