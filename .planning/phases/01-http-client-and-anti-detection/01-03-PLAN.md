---
phase: 01-http-client-and-anti-detection
plan: 03
type: execute
wave: 3
depends_on: ["01-02"]
files_modified:
  - tests/test_integration.py
autonomous: false

must_haves:
  truths:
    - "HLTVClient can fetch an HLTV match overview page and receive valid HTML with a 200 response"
    - "HLTVClient can fetch all 5 HLTV page types: results listing, match overview, map overview, map performance, map economy"
    - "HLTVClient waits a randomized delay between consecutive requests (observable in timing)"
    - "Fetching 20+ pages in sequence does not trigger Cloudflare challenge escalation"
    - "HLTVClient recovers from transient errors by retrying with exponential backoff"
  artifacts:
    - path: "tests/test_integration.py"
      provides: "Live integration tests against HLTV"
      contains: "HLTV_TEST_URLS"
      min_lines: 60
  key_links:
    - from: "tests/test_integration.py"
      to: "src/scraper/http_client.py"
      via: "imports and exercises HLTVClient.fetch()"
      pattern: "from scraper.http_client import HLTVClient"
---

<objective>
Validate the complete HTTP client against live HLTV pages. Fetch all 5 page types and run a 20-page sequence test to empirically confirm the anti-detection strategy works.

Purpose: This is the Phase 1 gate. If any page type returns Cloudflare challenges (especially the performance page), the phase is not complete. This test provides empirical data on HLTV's actual Cloudflare behavior.
Output: Passing integration tests proving the scraper can reliably fetch HLTV pages without getting blocked.
</objective>

<execution_context>
@~/.claude/get-shit-done/workflows/execute-plan.md
@~/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/01-http-client-and-anti-detection/01-RESEARCH.md
@.planning/phases/01-http-client-and-anti-detection/01-01-SUMMARY.md
@.planning/phases/01-http-client-and-anti-detection/01-02-SUMMARY.md
</context>

<tasks>

<task type="auto">
  <name>Task 1: Integration tests against live HLTV</name>
  <files>
    tests/test_integration.py
  </files>
  <action>
Create `tests/test_integration.py` with live integration tests. These tests make REAL HTTP requests to HLTV and take several minutes to run due to rate limiting delays.

Mark ALL tests in this file with `@pytest.mark.integration` so they can be run separately from unit tests:
```python
pytestmark = pytest.mark.integration
```

Also update `pyproject.toml` to add a pytest marker registration:
```toml
[tool.pytest.ini_options]
markers = ["integration: tests that make real HTTP requests to HLTV (slow)"]
```

**Test URLs** (use real, known-good HLTV URLs -- pick a major match from a recent tournament):

```python
HLTV_TEST_URLS = {
    "results_listing": "https://www.hltv.org/results",
    "match_overview": "https://www.hltv.org/matches/2376513/faze-vs-natus-vincere-blast-premier-spring-final-2025",
    "map_overview": "https://www.hltv.org/stats/matches/mapstatsid/178889/faze-vs-natus-vincere",
    "map_performance": "https://www.hltv.org/stats/matches/performance/mapstatsid/178889/faze-vs-natus-vincere",
    "map_economy": "https://www.hltv.org/stats/matches/economy/mapstatsid/178889/faze-vs-natus-vincere",
}
```

**Test 1: `test_all_page_types_reachable`**
- Create a single HLTVClient instance (single session, per research guidance)
- Fetch each of the 5 URLs
- For each response, validate:
  - Length > 1000 characters (not empty or stub)
  - Contains "hltv" or "HLTV" (is actually HLTV content, not a Cloudflare challenge page)
  - Does NOT contain "Just a moment..." or "Checking your browser" (not a challenge page)
- Print each result: page type, status, response length
- Fail with clear message if any page type fails, listing which ones
- The performance page is the critical gate -- if it fails, print a clear message: "PERFORMANCE PAGE BLOCKED: curl_cffi insufficient, need browser fallback"

**Test 2: `test_sequential_fetch_20_pages`**
- Create a single HLTVClient instance
- Build a list of 20+ URLs by combining:
  - 3 different results page offsets: `/results?offset=0`, `/results?offset=100`, `/results?offset=200`
  - 5+ different match overview pages (pick real match IDs from 2024-2025)
  - 5+ different map overview pages
  - 5+ map performance pages (the hardest ones)
  - 2+ map economy pages
- Fetch all URLs in sequence through the single client
- Track and print: total time, success count, failure count, average delay
- After completion, print `client.stats`
- Assert: success rate >= 90% (allow some transient failures, but not systematic blocking)
- Assert: no 3+ consecutive failures (would indicate escalating block)

**Test 3: `test_client_stats_tracking`**
- After fetching a few pages, verify `client.stats` returns correct counts for requests, successes, and current_delay

**Important implementation notes:**
- Use `print()` liberally -- these tests are diagnostic. When they run, the output tells us HLTV's actual behavior.
- If a test fails on the performance page, do NOT consider Phase 1 failed immediately. The test output should provide enough data to decide next steps (browser fallback vs. longer delays vs. different impersonation target).
- Set the test timeout to 10 minutes (`@pytest.mark.timeout(600)` if pytest-timeout is installed, otherwise just document that these are slow).
- Do NOT parallelize HTTP requests in tests -- sequential is the realistic usage pattern and what we need to validate.
  </action>
  <verify>
    Run `python -m pytest tests/test_integration.py -v -s -m integration --timeout=600` and observe the output. All 3 tests should pass. If performance/economy pages fail, note the specific error for the checkpoint.
  </verify>
  <done>
    Integration tests execute against live HLTV. All 5 page types return valid HTML. 20-page sequence completes with >= 90% success rate. Client stats are accurate.
  </done>
</task>

<task type="checkpoint:human-verify" gate="blocking">
  <what-built>
    Complete HTTP client with anti-detection, tested against live HLTV. The integration tests fetched all 5 HLTV page types (results listing, match overview, map overview, map performance, map economy) and ran a 20-page sequence test.
  </what-built>
  <how-to-verify>
    1. Review the integration test output (printed during test run)
    2. Confirm all 5 page types returned valid HTML (especially the performance page)
    3. Check that the 20-page sequence had >= 90% success rate
    4. Check that no consecutive failures suggest escalating blocks
    5. Review client.stats for request counts and current delay

    If performance/economy pages were blocked:
    - This is expected as a possibility (documented in research)
    - The decision is whether to implement SeleniumBase UC Mode fallback now or proceed and handle it later
    - Recommend: proceed to Phase 2 if overview/map_overview work, add browser fallback as a gap closure plan

    Run the tests yourself if desired:
    ```
    python -m pytest tests/test_integration.py -v -s -m integration
    ```
  </how-to-verify>
  <resume-signal>Type "approved" if all page types work, or describe which page types failed and whether to proceed or add a fallback plan.</resume-signal>
</task>

</tasks>

<verification>
The full test suite (unit + integration) validates the complete Phase 1 deliverable:

```bash
# Unit tests only (fast, no network)
python -m pytest tests/ -v --tb=short -m "not integration"

# Integration tests only (slow, hits HLTV)
python -m pytest tests/test_integration.py -v -s -m integration

# All tests
python -m pytest tests/ -v --tb=short
```

Phase 1 success criteria from ROADMAP.md:
1. Scraper can fetch HLTV match page -> 200 with valid HTML: VALIDATED by test_all_page_types_reachable
2. Randomized delay between requests: VALIDATED by rate limiter unit tests + observable timing in integration tests
3. Different User-Agent across requests: VALIDATED by UA rotator unit tests
4. Recovers from 403/429/503 with exponential backoff: VALIDATED by http_client unit tests (retry behavior)
5. 20+ pages without IP ban: VALIDATED by test_sequential_fetch_20_pages
</verification>

<success_criteria>
- All 5 HLTV page types return valid HTML (results, match overview, map overview, map performance, map economy)
- 20-page sequential fetch completes with >= 90% success rate
- No escalating Cloudflare blocks (no 3+ consecutive failures)
- Client stats accurately track request/success/challenge counts
- User has reviewed and approved the integration test results
</success_criteria>

<output>
After completion, create `.planning/phases/01-http-client-and-anti-detection/01-03-SUMMARY.md`
</output>
