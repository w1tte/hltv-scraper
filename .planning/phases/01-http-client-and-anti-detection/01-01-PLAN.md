---
phase: 01-http-client-and-anti-detection
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - pyproject.toml
  - src/scraper/__init__.py
  - src/scraper/config.py
  - src/scraper/exceptions.py
  - src/scraper/rate_limiter.py
  - src/scraper/user_agents.py
  - tests/__init__.py
  - tests/test_rate_limiter.py
  - tests/test_user_agents.py
autonomous: true

must_haves:
  truths:
    - "Project installs cleanly with pip install -e . and all dependencies resolve"
    - "RateLimiter enforces randomized delays between calls with jitter"
    - "RateLimiter adapts delay upward on backoff and downward on recover"
    - "UserAgentRotator produces Chrome-family UA strings consistent with curl_cffi impersonation target"
  artifacts:
    - path: "pyproject.toml"
      provides: "Project metadata, dependencies, Python version constraint"
      contains: "curl_cffi"
    - path: "src/scraper/config.py"
      provides: "All configurable values with sensible defaults"
      contains: "class ScraperConfig"
    - path: "src/scraper/exceptions.py"
      provides: "Custom exception hierarchy"
      contains: "CloudflareChallenge"
    - path: "src/scraper/rate_limiter.py"
      provides: "Adaptive rate limiter with jitter"
      contains: "class RateLimiter"
    - path: "src/scraper/user_agents.py"
      provides: "UA rotation matching impersonation target"
      contains: "class UserAgentRotator"
  key_links:
    - from: "src/scraper/rate_limiter.py"
      to: "src/scraper/config.py"
      via: "imports delay/backoff config values"
      pattern: "from.*config.*import|ScraperConfig"
    - from: "src/scraper/user_agents.py"
      to: "src/scraper/config.py"
      via: "imports impersonation target config"
      pattern: "from.*config.*import|ScraperConfig"
---

<objective>
Scaffold the HLTV scraper project and build the two independent support modules: RateLimiter (adaptive delay with jitter) and UserAgentRotator (Chrome-family UA rotation consistent with curl_cffi impersonation).

Purpose: Establish project structure, install dependencies, and create the foundational modules that the HTTP client will consume in Plan 02.
Output: Installable Python package with config, exceptions, rate limiter, and UA rotator -- all tested.
</objective>

<execution_context>
@~/.claude/get-shit-done/workflows/execute-plan.md
@~/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/01-http-client-and-anti-detection/01-RESEARCH.md
</context>

<tasks>

<task type="auto">
  <name>Task 1: Project scaffolding, dependencies, and foundation modules</name>
  <files>
    pyproject.toml
    src/scraper/__init__.py
    src/scraper/config.py
    src/scraper/exceptions.py
    tests/__init__.py
  </files>
  <action>
Create the Python project structure using src layout:

1. `pyproject.toml`:
   - Name: `hltv-scraper`
   - Python >= 3.11
   - Dependencies: `curl_cffi>=0.14.0`, `tenacity>=9.0`, `fake-useragent>=2.0`
   - Dev dependencies: `pytest>=8.0`, `pytest-cov`
   - Configure pytest (testpaths = ["tests"])

2. `src/scraper/__init__.py`:
   - Package init, export version string `__version__ = "0.1.0"`

3. `src/scraper/config.py`:
   - Create a `ScraperConfig` dataclass (use `@dataclass` from stdlib, NOT pydantic yet) with these fields and defaults:
     - `impersonate_target: str = "chrome136"` -- curl_cffi browser impersonation
     - `min_delay: float = 3.0` -- minimum seconds between requests
     - `max_delay: float = 8.0` -- maximum seconds (before jitter)
     - `backoff_factor: float = 2.0` -- multiplier on challenge/error
     - `recovery_factor: float = 0.95` -- multiplier on success (gradual recovery)
     - `max_backoff: float = 120.0` -- ceiling on delay
     - `max_retries: int = 5` -- tenacity stop_after_attempt
     - `timeout: int = 30` -- HTTP request timeout seconds
     - `base_url: str = "https://www.hltv.org"` -- hardcoded HLTV base
   - Add a `HLTV_BASE_URL` module-level constant = "https://www.hltv.org"

4. `src/scraper/exceptions.py`:
   - `HLTVScraperError(Exception)` -- base exception
   - `CloudflareChallenge(HLTVScraperError)` -- Cloudflare served challenge page
   - `RateLimited(HLTVScraperError)` -- HTTP 429
   - `HLTVFetchError(HLTVScraperError)` -- non-retriable fetch error (404, unexpected status)
   - `PageNotFound(HLTVFetchError)` -- HTTP 404 specifically
   - Each exception should store the `url` and `status_code` (optional) as attributes

5. `tests/__init__.py`: Empty file.

6. Run `pip install -e ".[dev]"` to install the project in editable mode. Verify all deps install cleanly.
  </action>
  <verify>
    Run `pip install -e ".[dev]"` and `python -c "from scraper.config import ScraperConfig; c = ScraperConfig(); print(c)"` and `python -c "from scraper.exceptions import CloudflareChallenge; raise CloudflareChallenge('test', url='http://x')"` (expect exception with url attribute).
  </verify>
  <done>
    Project installs, ScraperConfig instantiates with all defaults, all exception classes importable with url/status_code attributes.
  </done>
</task>

<task type="auto">
  <name>Task 2: Adaptive rate limiter with jitter and unit tests</name>
  <files>
    src/scraper/rate_limiter.py
    tests/test_rate_limiter.py
  </files>
  <action>
Create `src/scraper/rate_limiter.py`:

Implement `RateLimiter` class following the research pattern exactly:

```python
class RateLimiter:
    def __init__(self, config: ScraperConfig | None = None):
        # Pull min_delay, max_delay, backoff_factor, recovery_factor, max_backoff from config
        # If config is None, use ScraperConfig() defaults
```

Methods:
- `wait() -> float`: Sleep for a jittered delay. Jitter = uniform random in [current_delay, current_delay * 1.5]. Account for time already elapsed since last request (so if processing took 4s and delay is 5s, only sleep 1s). Track `_last_request_time` with `time.monotonic()`. Return the actual delay used.
- `backoff() -> None`: Multiply current_delay by backoff_factor, cap at max_backoff. Log a warning.
- `recover() -> None`: Multiply current_delay by recovery_factor, floor at min_delay.
- `reset() -> None`: Set current_delay back to min_delay.
- Property `current_delay -> float`: Expose current delay value.

Use `time.monotonic()` NOT `time.time()`. Use `logging.getLogger(__name__)` for the logger.

Create `tests/test_rate_limiter.py`:

Test the following (mock `time.sleep` and `time.monotonic` to avoid actual waits):
1. `test_wait_returns_jittered_delay` -- delay is between [current_delay, current_delay * 1.5]
2. `test_backoff_increases_delay` -- after backoff(), current_delay doubles (with default factor)
3. `test_backoff_caps_at_max` -- current_delay never exceeds max_backoff
4. `test_recover_decreases_delay` -- after recover(), current_delay decreases
5. `test_recover_floors_at_min` -- current_delay never drops below min_delay
6. `test_reset_returns_to_min` -- reset() sets current_delay to min_delay
7. `test_wait_accounts_for_elapsed_time` -- if 4s elapsed since last request and delay is 5s, sleep is ~1s
  </action>
  <verify>
    Run `python -m pytest tests/test_rate_limiter.py -v` -- all 7 tests pass.
  </verify>
  <done>
    RateLimiter class enforces randomized delays with jitter, adapts via backoff/recover, and all 7 unit tests pass.
  </done>
</task>

<task type="auto">
  <name>Task 3: User-Agent rotator with fingerprint consistency and unit tests</name>
  <files>
    src/scraper/user_agents.py
    tests/test_user_agents.py
  </files>
  <action>
Create `src/scraper/user_agents.py`:

Implement `UserAgentRotator` class:

```python
class UserAgentRotator:
    def __init__(self, impersonate_target: str = "chrome136"):
        # Parse browser family from target (chrome*, edge* -> "Chrome", safari* -> "Safari", firefox* -> "Firefox")
        # Initialize fake_useragent.UserAgent with browsers=[family], platforms=["desktop"], min_version=120.0
```

Methods:
- `get() -> str`: Return a random UA string from the matching browser family.
- `get_headers() -> dict[str, str]`: Return a dict with "User-Agent" key and, for Chrome family, also include "Sec-CH-UA-Platform": '"Windows"' and "Sec-CH-UA-Mobile": "?0". These Client Hints headers are sent by real Chrome browsers.

Static method:
- `_get_browser_family(target: str) -> str`: Map impersonation target prefix to browser family name. Default to "Chrome" for unknown targets.

CRITICAL CONSTRAINT (from research): When curl_cffi impersonates Chrome, the UA MUST be a Chrome-family string. A Firefox or Safari UA with a Chrome TLS fingerprint is instantly detected by Cloudflare. This class enforces that constraint by filtering fake-useragent to the matching browser family only.

NOTE on usage pattern (from research anti-patterns): Real browsers do NOT change User-Agent mid-session. The recommended pattern is to call `get()` once per session or per batch of 10-20 requests, NOT per request. Document this in the class docstring but do NOT enforce it in the class -- leave that to the HLTVClient in Plan 02.

Create `tests/test_user_agents.py`:

1. `test_chrome_target_returns_chrome_ua` -- UA string contains "Chrome/" when target is "chrome136"
2. `test_safari_target_returns_safari_ua` -- UA string contains "Safari" when target is "safari17_3"
3. `test_unknown_target_defaults_to_chrome` -- Unknown target falls back to Chrome family
4. `test_get_headers_includes_client_hints_for_chrome` -- Headers dict has Sec-CH-UA-Platform and Sec-CH-UA-Mobile for Chrome targets
5. `test_get_headers_no_client_hints_for_non_chrome` -- Safari/Firefox targets do NOT get Client Hints headers
6. `test_different_calls_can_return_different_uas` -- Call get() 10 times, at least 2 unique values (proves rotation works). Use a set to collect results.
  </action>
  <verify>
    Run `python -m pytest tests/test_user_agents.py -v` -- all 6 tests pass.
  </verify>
  <done>
    UserAgentRotator produces Chrome-family UAs matching the impersonation target, includes Client Hints headers for Chrome, and all 6 unit tests pass.
  </done>
</task>

</tasks>

<verification>
Run all tests and verify the full module structure:

```bash
python -m pytest tests/ -v --tb=short
python -c "from scraper.config import ScraperConfig; print(ScraperConfig())"
python -c "from scraper.rate_limiter import RateLimiter; r = RateLimiter(); print(f'delay: {r.current_delay}')"
python -c "from scraper.user_agents import UserAgentRotator; ua = UserAgentRotator(); print(ua.get())"
```

All commands succeed, all tests pass, no import errors.
</verification>

<success_criteria>
- pyproject.toml exists and `pip install -e .` succeeds with all dependencies
- ScraperConfig dataclass has all 9 fields with correct defaults
- Exception hierarchy: HLTVScraperError > CloudflareChallenge, RateLimited, HLTVFetchError > PageNotFound
- RateLimiter passes all 7 unit tests (jitter, backoff, recover, reset, elapsed accounting)
- UserAgentRotator passes all 6 unit tests (Chrome matching, Client Hints, rotation)
- No circular imports, all modules importable independently
</success_criteria>

<output>
After completion, create `.planning/phases/01-http-client-and-anti-detection/01-01-SUMMARY.md`
</output>
