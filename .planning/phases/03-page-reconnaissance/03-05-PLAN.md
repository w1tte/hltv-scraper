---
phase: 03-page-reconnaissance
plan: 05
type: execute
wave: 2
depends_on: ["03-01"]
files_modified:
  - .planning/phases/03-page-reconnaissance/recon/map-performance.md
autonomous: true

must_haves:
  truths:
    - "CSS selectors for all performance metrics (KPR, DPR, opening kills/deaths, multi-kill counts, clutches) are documented"
    - "Rating 3.0 field changes are documented with concrete HTML evidence: Round Swing replaces Impact, Multi-Kill Rating separated"
    - "Detection strategy for rating version is documented with specific selectors/indicators"
    - "Eco-adjusted stats toggle and its data source (initial HTML vs JavaScript-loaded) is documented"
    - "Selectors are verified programmatically against multiple saved HTML samples from different eras"
  artifacts:
    - path: ".planning/phases/03-page-reconnaissance/recon/map-performance.md"
      provides: "Complete selector map for HLTV performance sub-page with Rating 2.0/3.0 comparison"
      min_lines: 200
  key_links:
    - from: "recon/map-performance.md"
      to: "data/recon/performance-*.html.gz"
      via: "BeautifulSoup .select() verification"
      pattern: "soup\\.select"
---

<objective>
Analyze the HLTV map performance page (`/stats/matches/performance/mapstatsid/{id}/{slug}`) to produce a complete CSS selector map for all player performance metrics, with particular focus on documenting Rating 2.0/2.1 vs Rating 3.0 differences.

Purpose: This selector map addresses RECON-04 (rating version differences) and will be consumed by Phase 7 (Performance Extraction) to build the performance parser with dual-format handling.
Output: `.planning/phases/03-page-reconnaissance/recon/map-performance.md`
</objective>

<execution_context>
@~/.claude/get-shit-done/workflows/execute-plan.md
@~/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/phases/03-page-reconnaissance/03-RESEARCH.md
@.planning/phases/03-page-reconnaissance/03-01-SUMMARY.md
@.planning/phases/03-page-reconnaissance/recon/sample-manifest.md
</context>

<tasks>

<task type="auto">
  <name>Task 1: Programmatic selector discovery for performance page with rating version analysis</name>
  <files>.planning/phases/03-page-reconnaissance/recon/map-performance.md</files>
  <action>
Write a temporary analysis script that loads ALL performance page HTML samples from `data/recon/performance-*.html.gz` and systematically discovers and verifies CSS selectors using BeautifulSoup. Group samples by era to detect rating version differences.

**CRITICAL: This plan addresses RECON-04 (Rating version differences).** The analysis must explicitly compare performance page HTML from different eras.

**Analysis targets -- verify each selector against ALL performance page samples:**

**Section 1: Page-level metadata**
- Team names on this page
- Map name on this page
- Link back to main stats page
- Any metadata fields unique to the performance page

**Section 2: Performance stats table(s)**
- Stats table container (expected: `.stats-table` -- but may differ from map stats page)
- Table header row -- document ALL column headers EXACTLY as they appear
- For EACH column, document:
  - Player name and ID (same selectors as map stats, or different?)
  - KPR (Kills Per Round)
  - DPR (Deaths Per Round)
  - Impact rating (Rating 2.0/2.1 ONLY -- expected to be ABSENT on Rating 3.0 pages)
  - Round Swing (Rating 3.0 ONLY -- expected to be ABSENT on Rating 2.0/2.1 pages)
  - Multi-Kill Rating (Rating 3.0 ONLY -- separated from Impact)
  - KAST%
  - Opening kills (OK) / Opening deaths (OD) / Opening kill rating
  - Multi-kill round counts: 2k, 3k, 4k, 5k
  - Clutch stats: 1v1, 1v2, 1v3, 1v4, 1v5 wins
  - Any other columns present

- Table structure: one table per team or one combined table?
- Row count per table (should be 5 per team)

**Section 3: Overview/comparison table**
- Overview table (expected: `.overview-table`)
- Row labels (team-level aggregated stats)
- Team columns
- Document ALL rows and their values

**Section 4: Rating version detection (CRITICAL)**
Compare performance pages from different eras:
- **Early CS2 sample** (late 2023): What columns are present? Is "Impact" shown?
- **Mid-era sample** (mid 2024): Same question
- **Recent sample** (2025-2026, post-Rating 3.0 launch Aug 2025): What columns are present? Is "Round Swing" shown?

Since Rating 3.0 was retroactively applied to ALL matches (October 2025), even old match performance pages may now show 3.0 columns. **The analysis must determine:**
1. Do ALL performance pages now show Rating 3.0 columns regardless of match date? (If yes, the scraper may not need dual-format handling at all)
2. If not, what is the specific HTML difference? Show side-by-side column headers.
3. What is the most reliable detection signal? Propose a concrete detection strategy:
   - Presence/absence of specific column header text
   - Presence of eco-adjusted toggle
   - Data attributes on specific elements
   - CSS class differences

**Document the detection strategy with specific code pseudocode:**
```python
# Example detection strategy (to be refined based on actual HTML)
def detect_rating_version(soup):
    # Check for Rating 3.0 indicator
    if soup.select_one('.eco-adjusted-toggle'):
        return '3.0'
    if any('Round Swing' in th.text for th in soup.select('th')):
        return '3.0'
    if any('Impact' in th.text for th in soup.select('th')):
        return '2.0'
    return 'unknown'
```

**Section 5: Eco-adjusted stats (Rating 3.0 feature)**
- Is the eco-adjusted toggle present in the initial HTML? Or loaded via JavaScript?
- If present: selector for the toggle, selectors for eK-eD, eADR, eKAST columns
- If NOT present in initial HTML: document this clearly -- the scraper would need to interact with the toggle via nodriver to get this data
- Are eco-adjusted values in `data-*` attributes or hidden elements even if toggle is not clicked?

**Section 6: Other elements**
- Performance charts/graphs
- Navigation to other sub-pages (economy, etc.)
- Any elements unique to the performance page (not on map stats page)

**Annotated HTML snippets -- include for:**
- Stats table header row (showing ALL column headers)
- One player row (showing all data cells)
- Rating version comparison: side-by-side snippets from different eras (if they differ)
- Eco-adjusted toggle area (if present in HTML)

**Document ALL visible fields.** Mark with extraction status:
- **extract**: Phase 7 will parse this
- **skip**: Present but not needed
- **conditional**: Only present in certain rating versions (flag which version)
  </action>
  <verify>
The file `.planning/phases/03-page-reconnaissance/recon/map-performance.md` exists and contains:
1. Complete selector table for all performance metrics
2. Rating version comparison section with concrete HTML evidence
3. Detection strategy with pseudocode
4. Eco-adjusted stats section documenting toggle behavior and data availability
5. Annotated HTML snippets for key structures
6. Side-by-side comparison of column headers from different eras
7. Verification notes confirming selectors tested against all samples
8. Document is 200+ lines
  </verify>
  <done>Complete selector map for performance page exists, Rating 2.0 vs 3.0 differences documented with concrete HTML evidence, detection strategy proposed, eco-adjusted stats documented, all selectors verified against samples from multiple eras</done>
</task>

</tasks>

<verification>
1. `.planning/phases/03-page-reconnaissance/recon/map-performance.md` exists and is 200+ lines
2. All performance metrics have CSS selectors documented
3. Rating version section has concrete HTML evidence (column headers from different eras)
4. Detection strategy is specific and implementable (not vague)
5. Eco-adjusted stats toggle behavior is documented (in HTML vs JS-loaded)
6. Every selector tested against samples from at least 2 different eras
</verification>

<success_criteria>
- Every performance metric column has a documented CSS selector
- Rating 2.0/2.1 vs 3.0 differences are documented with concrete HTML evidence from actual samples
- A specific, implementable detection strategy for rating version is proposed
- Eco-adjusted stats data availability is determined (initial HTML vs JS-loaded)
- Document is detailed enough for Phase 7 to build a dual-format parser without needing to inspect HTML
</success_criteria>

<output>
After completion, create `.planning/phases/03-page-reconnaissance/03-05-SUMMARY.md`
</output>
