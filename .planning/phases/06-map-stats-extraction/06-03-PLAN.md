---
phase: 06-map-stats-extraction
plan: 03
type: execute
wave: 2
depends_on: ["06-01", "06-02"]
files_modified:
  - src/scraper/map_stats.py
  - tests/test_map_stats.py
autonomous: true

must_haves:
  truths:
    - "Orchestrator fetches map stats pages by mapstatsid URL, stores raw HTML, parses with parse_map_stats, and persists player_stats + round_history to DB"
    - "Fetch failures discard the entire batch; map entries remain pending for retry"
    - "Parse/persist failures are per-map: failed maps are logged, other maps in the batch continue"
    - "After successful processing, maps no longer appear in get_pending_map_stats results"
  artifacts:
    - path: "src/scraper/map_stats.py"
      provides: "Async orchestrator run_map_stats() function"
      exports: ["run_map_stats"]
    - path: "tests/test_map_stats.py"
      provides: "Orchestrator tests with mocked client and real DB"
      min_lines: 100
  key_links:
    - from: "src/scraper/map_stats.py"
      to: "src/scraper/map_stats_parser.py"
      via: "import parse_map_stats"
      pattern: "from scraper.map_stats_parser import parse_map_stats"
    - from: "src/scraper/map_stats.py"
      to: "src/scraper/repository.py"
      via: "match_repo.get_pending_map_stats() and match_repo.upsert_map_stats_complete()"
      pattern: "get_pending_map_stats|upsert_map_stats_complete"
    - from: "src/scraper/map_stats.py"
      to: "src/scraper/storage.py"
      via: "storage.save(html, match_id, page_type='map_stats', mapstatsid=X)"
      pattern: "storage\\.save.*map_stats"
---

<objective>
Create the async orchestrator that wires together fetching, storing, parsing, and persisting map stats data into the database.

Purpose: This is the integration layer that connects the parser (06-01) and repository (06-02) into a working pipeline. It follows the exact pattern of match_overview.py (Phase 5 orchestrator): fetch-first batching, per-item error handling, stats dict return.

Output: `src/scraper/map_stats.py` orchestrator with full test coverage using mocked client and real DB.
</objective>

<execution_context>
@~/.claude/get-shit-done/workflows/execute-plan.md
@~/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/06-map-stats-extraction/06-RESEARCH.md
@.planning/phases/06-map-stats-extraction/06-01-SUMMARY.md
@.planning/phases/06-map-stats-extraction/06-02-SUMMARY.md

Key references (read these during execution):
- src/scraper/match_overview.py -- Follow this EXACT pattern (fetch-first batch, per-entry error handling, stats dict)
- tests/test_match_overview.py -- Follow this EXACT test pattern (mock client, real DB, seed helpers)
- src/scraper/map_stats_parser.py -- Parser to call (created in 06-01)
- src/scraper/repository.py -- Repository methods to use (extended in 06-02)
- src/scraper/storage.py -- HtmlStorage.save/load with page_type="map_stats"
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create map_stats.py async orchestrator</name>
  <files>src/scraper/map_stats.py</files>
  <action>
Create `src/scraper/map_stats.py` following the EXACT pattern of `match_overview.py`.

**Module docstring:** "Map stats extraction orchestrator. Coordinates fetching, storing, parsing, and persisting per-map scoreboard and round history data."

**Constants:**
```python
PARSER_VERSION = "map_stats_v1"
MAP_STATS_URL_TEMPLATE = "/stats/matches/mapstatsid/{mapstatsid}/x"
```

Note on URL: The slug after the mapstatsid is cosmetic (HLTV routes by ID). Using "x" as a placeholder slug. If HLTV requires a valid slug, the orchestrator can construct one from team names in the DB, but the research suggests the numeric ID is sufficient.

**Main function:**
```python
async def run_map_stats(
    client,         # HLTVClient
    match_repo,     # MatchRepository
    storage,        # HtmlStorage
    config,         # ScraperConfig
) -> dict:
```

Note: Unlike match_overview.py which takes discovery_repo, this orchestrator only needs match_repo (for get_pending_map_stats and upsert_map_stats_complete) and does NOT need discovery_repo (no scrape_queue status to update).

**Implementation (follow match_overview.py step by step):**

1. Stats dict initialization:
```python
stats = {
    "batch_size": 0,
    "fetched": 0,
    "parsed": 0,
    "failed": 0,
    "fetch_errors": 0,
}
```

2. Get pending maps:
```python
pending = match_repo.get_pending_map_stats(limit=config.map_stats_batch_size)
stats["batch_size"] = len(pending)
```
Return early if empty.

3. Fetch phase -- fetch all map stats pages in the batch. On ANY fetch failure, discard entire batch (same pattern as match_overview.py):
```python
fetched_entries = []
for entry in pending:
    url = config.base_url + MAP_STATS_URL_TEMPLATE.format(mapstatsid=entry["mapstatsid"])
    try:
        html = await client.fetch(url)
        storage.save(html, match_id=entry["match_id"], page_type="map_stats", mapstatsid=entry["mapstatsid"])
        fetched_entries.append(entry)
    except Exception as exc:
        logger.error("Fetch failed for mapstatsid %d: %s. Discarding batch.", entry["mapstatsid"], exc)
        stats["fetch_errors"] += 1
        return stats
```

4. Parse + persist phase -- per-map error handling:
```python
for entry in fetched_entries:
    mapstatsid = entry["mapstatsid"]
    match_id = entry["match_id"]
    map_number = entry["map_number"]
    try:
        html = storage.load(match_id=match_id, page_type="map_stats", mapstatsid=mapstatsid)
        result = parse_map_stats(html, mapstatsid)

        now = datetime.now(timezone.utc).isoformat()
        source_url = config.base_url + MAP_STATS_URL_TEMPLATE.format(mapstatsid=mapstatsid)

        # Build player_stats dicts for UPSERT
        stats_data = []
        for ps in result.players:
            stats_data.append({
                "match_id": match_id,
                "map_number": map_number,
                "player_id": ps.player_id,
                "player_name": ps.player_name,
                "team_id": ps.team_id,
                "kills": ps.kills,
                "deaths": ps.deaths,
                "assists": ps.assists,
                "flash_assists": ps.flash_assists,
                "hs_kills": ps.hs_kills,
                "kd_diff": ps.kd_diff,
                "adr": ps.adr,
                "kast": ps.kast,
                "fk_diff": ps.fk_diff,
                "rating_2": ps.rating if ps.rating_version == "2.0" else None,
                "rating_3": ps.rating if ps.rating_version == "3.0" else None,
                "kpr": None,       # Phase 7
                "dpr": None,       # Phase 7
                "impact": None,    # Phase 7
                "scraped_at": now,
                "source_url": source_url,
                "parser_version": PARSER_VERSION,
            })

        # Build round_history dicts for UPSERT
        rounds_data = []
        for ro in result.rounds:
            rounds_data.append({
                "match_id": match_id,
                "map_number": map_number,
                "round_number": ro.round_number,
                "winner_side": ro.winner_side,
                "win_type": ro.win_type,
                "winner_team_id": ro.winner_team_id,
                "scraped_at": now,
                "source_url": source_url,
                "parser_version": PARSER_VERSION,
            })

        # Persist atomically
        match_repo.upsert_map_stats_complete(stats_data, rounds_data)
        stats["parsed"] += 1
        logger.info("Parsed and persisted mapstatsid %d (match %d, map %d)", mapstatsid, match_id, map_number)

    except Exception as exc:
        logger.error("Parse/persist failed for mapstatsid %d: %s", mapstatsid, exc)
        stats["failed"] += 1
```

5. Log batch summary and return stats.

**Key differences from match_overview.py:**
- No discovery_repo parameter (no scrape_queue status to update)
- Work items come from match_repo.get_pending_map_stats() instead of discovery_repo.get_pending_matches()
- URL uses MAP_STATS_URL_TEMPLATE with mapstatsid
- storage.save uses page_type="map_stats" with mapstatsid kwarg
- Persistence uses match_repo.upsert_map_stats_complete() instead of match_repo.upsert_match_overview()
- No scrape_queue status update on success/failure (processing state is derived from data presence)
  </action>
  <verify>
Run `python -c "from scraper.map_stats import run_map_stats; print('import OK')"` to confirm module imports.
  </verify>
  <done>
map_stats.py exists with run_map_stats() async function following the match_overview.py pattern exactly. Handles fetch-first batching, per-map error handling, atomic persistence via upsert_map_stats_complete.
  </done>
</task>

<task type="auto">
  <name>Task 2: Create orchestrator tests with mocked client and real DB</name>
  <files>tests/test_map_stats.py</files>
  <action>
Create `tests/test_map_stats.py` following the EXACT pattern of `tests/test_match_overview.py`.

**Fixtures (same pattern as test_match_overview.py):**

```python
@pytest.fixture
def tmp_db(tmp_path):
    db = Database(tmp_path / "test.db")
    db.initialize()
    return db

@pytest.fixture
def match_repo(tmp_db):
    return MatchRepository(tmp_db.conn)

@pytest.fixture
def storage(tmp_path):
    return HtmlStorage(tmp_path / "raw")

@pytest.fixture
def config(tmp_path):
    return ScraperConfig(data_dir=str(tmp_path), map_stats_batch_size=2)

@pytest.fixture
def mock_client():
    client = MagicMock()
    client.fetch = AsyncMock()
    return client
```

**Seed helper:**
```python
def seed_match_with_maps(match_repo, match_id, mapstatsids):
    """Insert a match and maps into the DB so get_pending_map_stats finds them."""
```
This must insert a match into matches table AND maps into maps table with the given mapstatsids. Use minimal valid data for the match (match_id, date, team1_id, team1_name, team2_id, team2_name, scraped_at, source_url, parser_version -- other fields can be None or default). For each mapstatsid, insert a map row (match_id, map_number=1-based index, mapstatsid, scraped_at, source_url, parser_version).

Use `match_repo.conn.execute()` directly (with `match_repo.conn.commit()`) since there is no convenience method for seeding test data at this level.

**HTML sample:** Use `mapstats-164779-stats.html.gz` from data/recon/ as the test HTML. Load with the same load_sample helper pattern from test_match_overview.py.

Also need a match_id that would plausibly own mapstatsid 164779. Use match_id=2371389 (or any test value -- the parser doesn't validate match_id against the HTML, it just passes through).

**Test class TestRunMapStats:**

1. `test_fetches_parses_persists_map_stats`:
   - Seed match with 1 map (mapstatsid=164779)
   - Mock client.fetch returns real HTML from sample
   - Run orchestrator
   - Verify stats: batch_size=1, fetched=1, parsed=1, failed=0
   - Verify player_stats rows exist: match_repo.get_player_stats(match_id, map_number=1) returns 10 rows
   - Verify round_history exists: query DB for round_history rows with that match_id+map_number, count > 0
   - Verify map no longer pending: match_repo.get_pending_map_stats() returns empty

2. `test_no_pending_maps_returns_early`:
   - Don't seed anything
   - Run orchestrator
   - Verify stats: batch_size=0, client.fetch never called

3. `test_fetch_failure_discards_batch`:
   - Seed match with 2 maps
   - Mock client.fetch: first succeeds, second raises Exception
   - Run orchestrator
   - Verify stats: fetch_errors=1, parsed=0
   - Verify no player_stats or round_history in DB
   - Verify both maps still pending (get_pending_map_stats returns 2)

4. `test_parse_failure_continues_batch`:
   - Seed match with 2 maps (mapstatsid_a=164779, mapstatsid_b=999999)
   - Mock client.fetch: first returns real HTML, second returns garbage HTML
   - Run orchestrator
   - Verify stats: fetched=2, parsed=1, failed=1
   - Verify player_stats exist for the successfully parsed map
   - Verify the failed map still appears in pending (has no player_stats)

5. `test_raw_html_stored`:
   - Seed match with 1 map
   - Mock client.fetch returns real HTML
   - Run orchestrator
   - Verify storage.exists(match_id, page_type="map_stats", mapstatsid=164779) is True
   - Verify storage.load returns the HTML

6. `test_stats_dict_has_all_keys`:
   - Run orchestrator (empty batch is fine)
   - Verify returned dict has keys: batch_size, fetched, parsed, failed, fetch_errors

Run: `pytest tests/test_map_stats.py -v`

All tests must pass. If any fail due to integration issues between parser/repo/orchestrator, debug and fix.
  </action>
  <verify>
`pytest tests/test_map_stats.py -v` -- all tests pass, 0 failures, 0 errors.
  </verify>
  <done>
Orchestrator tests verify full pipeline: fetch-store-parse-persist, batch discard on fetch failure, per-map error handling on parse failure, raw HTML storage, and empty batch early return. All tests pass with mocked client and real in-memory DB.
  </done>
</task>

</tasks>

<verification>
1. `python -c "from scraper.map_stats import run_map_stats"` succeeds
2. `pytest tests/test_map_stats.py -v` -- all tests pass
3. `pytest tests/test_map_stats_parser.py tests/test_map_stats.py tests/test_repository.py -v` -- full Phase 6 test suite passes
4. `pytest tests/ -m "not integration" -v` -- entire non-integration test suite passes (no regressions)
</verification>

<success_criteria>
- run_map_stats() fetches, stores, parses, and persists map stats data for pending maps
- Fetch failures discard entire batch (maps remain pending for retry)
- Parse failures are per-map (failed map logged, others continue)
- After successful processing, maps are no longer returned by get_pending_map_stats
- Raw HTML is stored before parsing (fetch-first pattern)
- All Phase 6 tests pass, no regressions in existing test suite
</success_criteria>

<output>
After completion, create `.planning/phases/06-map-stats-extraction/06-03-SUMMARY.md`
</output>
